{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "As a baseline solution we used this public [notebook](https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train)"
      ],
      "metadata": {
        "id": "wEAnw8l6Bitb"
      },
      "id": "wEAnw8l6Bitb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366bd7ae",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:22:16.255324Z",
          "iopub.status.busy": "2022-11-24T09:22:16.255036Z",
          "iopub.status.idle": "2022-11-24T09:22:19.303178Z",
          "shell.execute_reply": "2022-11-24T09:22:19.302058Z"
        },
        "papermill": {
          "duration": 3.059178,
          "end_time": "2022-11-24T09:22:19.305613",
          "exception": false,
          "start_time": "2022-11-24T09:22:16.246435",
          "status": "completed"
        },
        "tags": [],
        "id": "366bd7ae",
        "outputId": "6b9c57da-eb69-4858-e552-8af8cb36fbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: allennlp 2.10.1\r\n",
            "Uninstalling allennlp-2.10.1:\r\n",
            "  Successfully uninstalled allennlp-2.10.1\r\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# For right dependencies on kaggle this library was uninstalled\n",
        "!pip uninstall allennlp -y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd2e260f",
      "metadata": {
        "papermill": {
          "duration": 0.007883,
          "end_time": "2022-11-24T09:22:19.321843",
          "exception": false,
          "start_time": "2022-11-24T09:22:19.313960",
          "status": "completed"
        },
        "tags": [],
        "id": "bd2e260f"
      },
      "source": [
        "### Global settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af762f76",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:22:19.341187Z",
          "iopub.status.busy": "2022-11-24T09:22:19.340196Z",
          "iopub.status.idle": "2022-11-24T09:22:19.346031Z",
          "shell.execute_reply": "2022-11-24T09:22:19.345190Z"
        },
        "papermill": {
          "duration": 0.018161,
          "end_time": "2022-11-24T09:22:19.347958",
          "exception": false,
          "start_time": "2022-11-24T09:22:19.329797",
          "status": "completed"
        },
        "tags": [],
        "id": "af762f76"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "OUTPUT_DIR = './'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a176ad82",
      "metadata": {
        "papermill": {
          "duration": 0.007787,
          "end_time": "2022-11-24T09:22:19.363968",
          "exception": false,
          "start_time": "2022-11-24T09:22:19.356181",
          "status": "completed"
        },
        "tags": [],
        "id": "a176ad82"
      },
      "source": [
        "# CFG"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main points concerning the architecture:\n",
        "* We use pretrained weigts of deberta-v3-base model \n",
        "* We fine-tune the last linear layer in respect to 6 desired metrics\n",
        "* Training via 4-fold cross-validation\n",
        "* Averaging the score using model from each fold"
      ],
      "metadata": {
        "id": "BTTTH78oB-Hp"
      },
      "id": "BTTTH78oB-Hp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54bce907",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:22:19.381533Z",
          "iopub.status.busy": "2022-11-24T09:22:19.380852Z",
          "iopub.status.idle": "2022-11-24T09:22:19.388086Z",
          "shell.execute_reply": "2022-11-24T09:22:19.387117Z"
        },
        "papermill": {
          "duration": 0.017957,
          "end_time": "2022-11-24T09:22:19.389992",
          "exception": false,
          "start_time": "2022-11-24T09:22:19.372035",
          "status": "completed"
        },
        "tags": [],
        "id": "54bce907"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# CFG\n",
        "# ====================================================\n",
        "class CFG:\n",
        "#     wandb=True\n",
        "    wandb=False\n",
        "    competition='FB3'\n",
        "    _wandb_kernel='wallykop'\n",
        "    debug=False\n",
        "    apex=True\n",
        "    print_freq=20\n",
        "    num_workers=4\n",
        "    model=\"/kaggle/input/debertav3base/\"\n",
        "    gradient_checkpointing=True\n",
        "    scheduler='cosine' # ['linear', 'cosine']\n",
        "    batch_scheduler=True\n",
        "    num_cycles=0.5\n",
        "    num_warmup_steps=0\n",
        "    epochs=5\n",
        "    encoder_lr=2e-5\n",
        "    decoder_lr=2e-5\n",
        "    min_lr=1e-6\n",
        "    eps=1e-6\n",
        "    betas=(0.9, 0.999)\n",
        "    batch_size=8\n",
        "    max_len=512\n",
        "    weight_decay=0.01\n",
        "    gradient_accumulation_steps=1\n",
        "    max_grad_norm=1000\n",
        "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "    seed=2807\n",
        "    n_fold=4\n",
        "    trn_fold=[0, 1, 2, 3]\n",
        "    train=True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf1845e5",
      "metadata": {
        "papermill": {
          "duration": 0.007778,
          "end_time": "2022-11-24T09:22:19.431513",
          "exception": false,
          "start_time": "2022-11-24T09:22:19.423735",
          "status": "completed"
        },
        "tags": [],
        "id": "bf1845e5"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f97faaa7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:22:19.448969Z",
          "iopub.status.busy": "2022-11-24T09:22:19.448214Z",
          "iopub.status.idle": "2022-11-24T09:22:59.542220Z",
          "shell.execute_reply": "2022-11-24T09:22:59.540946Z"
        },
        "papermill": {
          "duration": 40.105509,
          "end_time": "2022-11-24T09:22:59.544989",
          "exception": false,
          "start_time": "2022-11-24T09:22:19.439480",
          "status": "completed"
        },
        "tags": [],
        "id": "f97faaa7",
        "outputId": "487eeee1-1e56-43cb-98a2-4c5375457689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: ../input/fb3-pip-wheels\n",
            "Processing /kaggle/input/fb3-pip-wheels/iterative_stratification-0.1.7-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification) (1.0.2)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification) (3.1.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.20.1\n",
            "Uninstalling transformers-4.20.1:\n",
            "  Successfully uninstalled transformers-4.20.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tokenizers 0.12.1\n",
            "Uninstalling tokenizers-0.12.1:\n",
            "  Successfully uninstalled tokenizers-0.12.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: ../input/fb3-pip-wheels\n",
            "Processing /kaggle/input/fb3-pip-wheels/transformers-4.21.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\n",
            "Processing /kaggle/input/fb3-pip-wheels/tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: ../input/fb3-pip-wheels\n",
            "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.12.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenizers.__version__: 0.12.1\n",
            "transformers.__version__: 4.21.2\n",
            "env: TOKENIZERS_PARALLELISM=true\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "import ast\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import string\n",
        "import pickle\n",
        "import random\n",
        "import joblib\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
        "\n",
        "os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels iterative-stratification')\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "os.system('pip uninstall -y transformers')\n",
        "os.system('pip uninstall -y tokenizers')\n",
        "os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels transformers')\n",
        "os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels tokenizers')\n",
        "import tokenizers\n",
        "import transformers\n",
        "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
        "print(f\"transformers.__version__: {transformers.__version__}\")\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
        "%env TOKENIZERS_PARALLELISM=true\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b7f9a4c",
      "metadata": {
        "papermill": {
          "duration": 0.010578,
          "end_time": "2022-11-24T09:22:59.566703",
          "exception": false,
          "start_time": "2022-11-24T09:22:59.556125",
          "status": "completed"
        },
        "tags": [],
        "id": "7b7f9a4c"
      },
      "source": [
        "## Definition of utils functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c1127b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:22:59.587898Z",
          "iopub.status.busy": "2022-11-24T09:22:59.587366Z",
          "iopub.status.idle": "2022-11-24T09:22:59.601159Z",
          "shell.execute_reply": "2022-11-24T09:22:59.600225Z"
        },
        "papermill": {
          "duration": 0.026186,
          "end_time": "2022-11-24T09:22:59.603065",
          "exception": false,
          "start_time": "2022-11-24T09:22:59.576879",
          "status": "completed"
        },
        "tags": [],
        "id": "77c1127b"
      },
      "outputs": [],
      "source": [
        "from utils import (\n",
        "    MCRMSE,\n",
        "    get_score,\n",
        "    get_logger,\n",
        "    seed_everything\n",
        ")\n",
        "\n",
        "LOGGER = get_logger()  \n",
        "seed_everything(seed=2807)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ffba5ee",
      "metadata": {
        "papermill": {
          "duration": 0.008825,
          "end_time": "2022-11-24T09:22:59.621272",
          "exception": false,
          "start_time": "2022-11-24T09:22:59.612447",
          "status": "completed"
        },
        "tags": [],
        "id": "3ffba5ee"
      },
      "source": [
        "# Data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d958bd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:22:59.641202Z",
          "iopub.status.busy": "2022-11-24T09:22:59.640453Z",
          "iopub.status.idle": "2022-11-24T09:22:59.898355Z",
          "shell.execute_reply": "2022-11-24T09:22:59.897288Z"
        },
        "papermill": {
          "duration": 0.271317,
          "end_time": "2022-11-24T09:22:59.901486",
          "exception": false,
          "start_time": "2022-11-24T09:22:59.630169",
          "status": "completed"
        },
        "tags": [],
        "id": "30d958bd",
        "outputId": "9201908b-1efe-41c7-c6d1-8cf1693bc7ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train.shape: (3911, 8)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0016926B079C</td>\n",
              "      <td>I think that students would benefit from learn...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0022683E9EA5</td>\n",
              "      <td>When a problem is a change you have to let it ...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00299B378633</td>\n",
              "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003885A45F42</td>\n",
              "      <td>The best time in life is when you become yours...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0049B1DF5CCC</td>\n",
              "      <td>Small act of kindness can impact in other peop...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id                                          full_text  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
              "0  0016926B079C  I think that students would benefit from learn...       3.5     3.5         3.0          3.0      4.0          3.0\n",
              "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5     2.5         3.0          2.0      2.0          2.5\n",
              "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0     3.5         3.0          3.0      3.0          2.5\n",
              "3  003885A45F42  The best time in life is when you become yours...       4.5     4.5         4.5          4.5      4.0          5.0\n",
              "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5     3.0         3.0          3.0      2.5          2.5"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test.shape: (3, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>when a person has no experience on a job their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>Do you think students would benefit from being...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>Thomas Jefferson once states that \"it is wonde...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id                                          full_text\n",
              "0  0000C359D63E  when a person has no experience on a job their...\n",
              "1  000BAD50D026  Do you think students would benefit from being...\n",
              "2  00367BB2546B  Thomas Jefferson once states that \"it is wonde..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submission.shape: (3, 7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
              "0  0000C359D63E       3.0     3.0         3.0          3.0      3.0          3.0\n",
              "1  000BAD50D026       3.0     3.0         3.0          3.0      3.0          3.0\n",
              "2  00367BB2546B       3.0     3.0         3.0          3.0      3.0          3.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ====================================================\n",
        "# Data Loading\n",
        "# ====================================================\n",
        "train = pd.read_csv('../input/feedback-prize-english-language-learning/train.csv')\n",
        "test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n",
        "submission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n",
        "\n",
        "print(f\"train.shape: {train.shape}\")\n",
        "display(train.head())\n",
        "print(f\"test.shape: {test.shape}\")\n",
        "display(test.head())\n",
        "print(f\"submission.shape: {submission.shape}\")\n",
        "display(submission.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "542905fd",
      "metadata": {
        "papermill": {
          "duration": 0.010004,
          "end_time": "2022-11-24T09:22:59.922597",
          "exception": false,
          "start_time": "2022-11-24T09:22:59.912593",
          "status": "completed"
        },
        "tags": [],
        "id": "542905fd"
      },
      "source": [
        "# CV split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c81d2d5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:22:59.943440Z",
          "iopub.status.busy": "2022-11-24T09:22:59.943002Z",
          "iopub.status.idle": "2022-11-24T09:23:00.067044Z",
          "shell.execute_reply": "2022-11-24T09:23:00.066166Z"
        },
        "papermill": {
          "duration": 0.136569,
          "end_time": "2022-11-24T09:23:00.069076",
          "exception": false,
          "start_time": "2022-11-24T09:22:59.932507",
          "status": "completed"
        },
        "tags": [],
        "id": "2c81d2d5",
        "outputId": "f74629a3-0a9d-46c9-edfb-92b413abb245"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "fold\n",
              "0    977\n",
              "1    978\n",
              "2    978\n",
              "3    978\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ====================================================\n",
        "# CV split\n",
        "# ====================================================\n",
        "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
        "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n",
        "    train.loc[val_index, 'fold'] = int(n)\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "display(train.groupby('fold').size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ecc39a",
      "metadata": {
        "papermill": {
          "duration": 0.009837,
          "end_time": "2022-11-24T09:23:00.116636",
          "exception": false,
          "start_time": "2022-11-24T09:23:00.106799",
          "status": "completed"
        },
        "tags": [],
        "id": "b8ecc39a"
      },
      "source": [
        "# Deberta tokenizer load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9989f5c5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:23:00.137209Z",
          "iopub.status.busy": "2022-11-24T09:23:00.136402Z",
          "iopub.status.idle": "2022-11-24T09:23:01.373395Z",
          "shell.execute_reply": "2022-11-24T09:23:01.372391Z"
        },
        "papermill": {
          "duration": 1.249799,
          "end_time": "2022-11-24T09:23:01.375922",
          "exception": false,
          "start_time": "2022-11-24T09:23:00.126123",
          "status": "completed"
        },
        "tags": [],
        "id": "9989f5c5",
        "outputId": "29816e89-7c31-488f-b069-9996988900be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# ====================================================\n",
        "# tokenizer\n",
        "# ====================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.model, local_files_only=True)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
        "CFG.tokenizer = tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c30353c",
      "metadata": {
        "papermill": {
          "duration": 0.009573,
          "end_time": "2022-11-24T09:23:01.395521",
          "exception": false,
          "start_time": "2022-11-24T09:23:01.385948",
          "status": "completed"
        },
        "tags": [],
        "id": "0c30353c"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a9e594",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:23:01.416807Z",
          "iopub.status.busy": "2022-11-24T09:23:01.416342Z",
          "iopub.status.idle": "2022-11-24T09:23:07.242623Z",
          "shell.execute_reply": "2022-11-24T09:23:07.241605Z"
        },
        "papermill": {
          "duration": 5.839769,
          "end_time": "2022-11-24T09:23:07.245184",
          "exception": false,
          "start_time": "2022-11-24T09:23:01.405415",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "ef28521a97e644efaa0ab84ab3959f72"
          ]
        },
        "id": "d6a9e594",
        "outputId": "bfdc98a7-2884-4420-99eb-05db1ab6048d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef28521a97e644efaa0ab84ab3959f72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3911 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_len: 1429\n"
          ]
        }
      ],
      "source": [
        "# ====================================================\n",
        "# Define max_len\n",
        "# ====================================================\n",
        "lengths = []\n",
        "tk0 = tqdm(train['full_text'].fillna(\"\").values, total=len(train))\n",
        "for text in tk0:\n",
        "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
        "    lengths.append(length)\n",
        "CFG.max_len = max(lengths) + 3 # cls & sep & sep\n",
        "LOGGER.info(f\"max_len: {CFG.max_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060770f5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:23:07.266759Z",
          "iopub.status.busy": "2022-11-24T09:23:07.266469Z",
          "iopub.status.idle": "2022-11-24T09:23:07.276398Z",
          "shell.execute_reply": "2022-11-24T09:23:07.275516Z"
        },
        "papermill": {
          "duration": 0.023121,
          "end_time": "2022-11-24T09:23:07.278605",
          "exception": false,
          "start_time": "2022-11-24T09:23:07.255484",
          "status": "completed"
        },
        "tags": [],
        "id": "060770f5"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Dataset\n",
        "# ====================================================\n",
        "from dataset import (\n",
        "    prepare_input,\n",
        "    TrainDataset,\n",
        "    collate\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e34910",
      "metadata": {
        "papermill": {
          "duration": 0.009624,
          "end_time": "2022-11-24T09:23:07.298023",
          "exception": false,
          "start_time": "2022-11-24T09:23:07.288399",
          "status": "completed"
        },
        "tags": [],
        "id": "f6e34910"
      },
      "source": [
        "# Model import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d362952",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:23:07.318894Z",
          "iopub.status.busy": "2022-11-24T09:23:07.318624Z",
          "iopub.status.idle": "2022-11-24T09:23:07.332631Z",
          "shell.execute_reply": "2022-11-24T09:23:07.331737Z"
        },
        "papermill": {
          "duration": 0.026741,
          "end_time": "2022-11-24T09:23:07.334561",
          "exception": false,
          "start_time": "2022-11-24T09:23:07.307820",
          "status": "completed"
        },
        "tags": [],
        "id": "5d362952"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Model\n",
        "# ====================================================\n",
        "from model import (\n",
        "    MeanPooling,\n",
        "    CustomModel\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9200e628",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:23:07.375700Z",
          "iopub.status.busy": "2022-11-24T09:23:07.374855Z",
          "iopub.status.idle": "2022-11-24T09:23:07.381750Z",
          "shell.execute_reply": "2022-11-24T09:23:07.380923Z"
        },
        "papermill": {
          "duration": 0.019789,
          "end_time": "2022-11-24T09:23:07.383645",
          "exception": false,
          "start_time": "2022-11-24T09:23:07.363856",
          "status": "completed"
        },
        "tags": [],
        "id": "9200e628"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Loss\n",
        "# ====================================================\n",
        "from model import RMSELoss\n",
        "\n",
        "# ====================================================\n",
        "# Helper functions\n",
        "# ====================================================\n",
        "from model import AverageMeter\n",
        "from utils import asMinutes, timeSince\n",
        "from main_functions import train_fn, valid_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe7384d",
      "metadata": {
        "papermill": {
          "duration": 0.009597,
          "end_time": "2022-11-24T09:23:07.464567",
          "exception": false,
          "start_time": "2022-11-24T09:23:07.454970",
          "status": "completed"
        },
        "tags": [],
        "id": "ffe7384d"
      },
      "source": [
        "# train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d70c2b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:23:07.485478Z",
          "iopub.status.busy": "2022-11-24T09:23:07.485185Z",
          "iopub.status.idle": "2022-11-24T09:23:07.503194Z",
          "shell.execute_reply": "2022-11-24T09:23:07.502391Z"
        },
        "papermill": {
          "duration": 0.030821,
          "end_time": "2022-11-24T09:23:07.505150",
          "exception": false,
          "start_time": "2022-11-24T09:23:07.474329",
          "status": "completed"
        },
        "tags": [],
        "id": "d9d70c2b"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# train loop\n",
        "# ====================================================\n",
        "def train_loop(folds, fold):\n",
        "    \n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "    # ====================================================\n",
        "    # loader\n",
        "    # ====================================================\n",
        "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
        "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
        "    valid_labels = valid_folds[CFG.target_cols].values\n",
        "    \n",
        "    train_dataset = TrainDataset(CFG, train_folds)\n",
        "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=CFG.batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset,\n",
        "                              batch_size=CFG.batch_size * 2,\n",
        "                              shuffle=False,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
        "\n",
        "    # ====================================================\n",
        "    # model & optimizer\n",
        "    # ====================================================\n",
        "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
        "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
        "    model.to(device)\n",
        "    \n",
        "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_parameters = [\n",
        "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
        "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
        "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
        "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        return optimizer_parameters\n",
        "\n",
        "    optimizer_parameters = get_optimizer_params(model,\n",
        "                                                encoder_lr=CFG.encoder_lr, \n",
        "                                                decoder_lr=CFG.decoder_lr,\n",
        "                                                weight_decay=CFG.weight_decay)\n",
        "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
        "    \n",
        "    # ====================================================\n",
        "    # scheduler\n",
        "    # ====================================================\n",
        "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
        "        if cfg.scheduler == 'linear':\n",
        "            scheduler = get_linear_schedule_with_warmup(\n",
        "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
        "            )\n",
        "        elif cfg.scheduler == 'cosine':\n",
        "            scheduler = get_cosine_schedule_with_warmup(\n",
        "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
        "            )\n",
        "        return scheduler\n",
        "    \n",
        "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
        "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
        "\n",
        "    # ====================================================\n",
        "    # loop\n",
        "    # ====================================================\n",
        "    criterion = nn.SmoothL1Loss(reduction='mean') # RMSELoss(reduction=\"mean\")\n",
        "    \n",
        "    best_score = np.inf\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # train\n",
        "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
        "\n",
        "        # eval\n",
        "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
        "        \n",
        "        # scoring\n",
        "        score, scores = get_score(valid_labels, predictions)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
        "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
        "        \n",
        "        if best_score > score:\n",
        "            best_score = score\n",
        "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
        "            torch.save({'model': model.state_dict(),\n",
        "                        'predictions': predictions},\n",
        "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
        "\n",
        "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
        "                             map_location=torch.device('cpu'))['predictions']\n",
        "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    return valid_folds\n",
        "\n",
        "  def get_result(oof_df):\n",
        "      labels = oof_df[CFG.target_cols].values\n",
        "      preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
        "      score, scores = get_score(labels, preds)\n",
        "      LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d9e165",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T09:23:07.526286Z",
          "iopub.status.busy": "2022-11-24T09:23:07.525550Z",
          "iopub.status.idle": "2022-11-24T13:29:30.751503Z",
          "shell.execute_reply": "2022-11-24T13:29:30.750576Z"
        },
        "papermill": {
          "duration": 14783.238754,
          "end_time": "2022-11-24T13:29:30.753699",
          "exception": false,
          "start_time": "2022-11-24T09:23:07.514945",
          "status": "completed"
        },
        "tags": [],
        "id": "89d9e165",
        "outputId": "7c822584-4bc6-4b5f-ae30-ed43ddd0f5a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "========== fold: 0 training ==========\n",
            "DebertaV2Config {\n",
            "  \"_name_or_path\": \"/kaggle/input/debertav3base/\",\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at /kaggle/input/debertav3base/ were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/366] Elapsed 0m 2s (remain 17m 14s) Loss: 2.4604(2.4604) Grad: inf  LR: 0.00002000  \n",
            "Epoch: [1][20/366] Elapsed 0m 44s (remain 12m 3s) Loss: 0.2929(1.4000) Grad: 132482.9688  LR: 0.00001999  \n",
            "Epoch: [1][40/366] Elapsed 1m 15s (remain 9m 58s) Loss: 0.1745(0.8305) Grad: 94354.8281  LR: 0.00001998  \n",
            "Epoch: [1][60/366] Elapsed 1m 54s (remain 9m 33s) Loss: 0.1684(0.6046) Grad: 65162.9609  LR: 0.00001995  \n",
            "Epoch: [1][80/366] Elapsed 2m 35s (remain 9m 7s) Loss: 0.1224(0.4876) Grad: 82596.6719  LR: 0.00001990  \n",
            "Epoch: [1][100/366] Elapsed 3m 12s (remain 8m 26s) Loss: 0.1240(0.4257) Grad: 76056.7812  LR: 0.00001985  \n",
            "Epoch: [1][120/366] Elapsed 3m 51s (remain 7m 48s) Loss: 0.1067(0.3777) Grad: 96725.1562  LR: 0.00001979  \n",
            "Epoch: [1][140/366] Elapsed 4m 30s (remain 7m 12s) Loss: 0.1338(0.3427) Grad: 66807.3359  LR: 0.00001971  \n",
            "Epoch: [1][160/366] Elapsed 5m 6s (remain 6m 30s) Loss: 0.2027(0.3150) Grad: 127323.6328  LR: 0.00001962  \n",
            "Epoch: [1][180/366] Elapsed 5m 39s (remain 5m 47s) Loss: 0.2016(0.2974) Grad: 136096.4062  LR: 0.00001952  \n",
            "Epoch: [1][200/366] Elapsed 6m 16s (remain 5m 8s) Loss: 0.1074(0.2814) Grad: 36069.6133  LR: 0.00001941  \n",
            "Epoch: [1][220/366] Elapsed 6m 53s (remain 4m 31s) Loss: 0.1191(0.2667) Grad: 68395.4766  LR: 0.00001929  \n",
            "Epoch: [1][240/366] Elapsed 7m 30s (remain 3m 53s) Loss: 0.1319(0.2548) Grad: 19768.7324  LR: 0.00001916  \n",
            "Epoch: [1][260/366] Elapsed 8m 5s (remain 3m 15s) Loss: 0.0957(0.2453) Grad: 47239.7891  LR: 0.00001902  \n",
            "Epoch: [1][280/366] Elapsed 8m 42s (remain 2m 38s) Loss: 0.1411(0.2362) Grad: 53952.3477  LR: 0.00001886  \n",
            "Epoch: [1][300/366] Elapsed 9m 18s (remain 2m 0s) Loss: 0.0923(0.2284) Grad: 52536.3711  LR: 0.00001870  \n",
            "Epoch: [1][320/366] Elapsed 9m 52s (remain 1m 23s) Loss: 0.1798(0.2219) Grad: 87793.8984  LR: 0.00001852  \n",
            "Epoch: [1][340/366] Elapsed 10m 25s (remain 0m 45s) Loss: 0.1722(0.2158) Grad: 48909.8672  LR: 0.00001834  \n",
            "Epoch: [1][360/366] Elapsed 11m 1s (remain 0m 9s) Loss: 0.1274(0.2109) Grad: 22009.3105  LR: 0.00001815  \n",
            "Epoch: [1][365/366] Elapsed 11m 8s (remain 0m 0s) Loss: 0.0726(0.2093) Grad: 42188.6094  LR: 0.00001810  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 15s) Loss: 0.1073(0.1073) \n",
            "EVAL: [20/62] Elapsed 0m 23s (remain 0m 46s) Loss: 0.1002(0.1204) \n",
            "EVAL: [40/62] Elapsed 0m 47s (remain 0m 24s) Loss: 0.1171(0.1162) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2093  avg_val_loss: 0.1172  time: 739s\n",
            "Epoch 1 - Score: 0.4852  Scores: [0.5252723293837455, 0.4603949603507831, 0.4324231774772289, 0.46372561913896837, 0.49361884898336805, 0.5359030621072157]\n",
            "Epoch 1 - Save Best Score: 0.4852 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [60/62] Elapsed 1m 10s (remain 0m 1s) Loss: 0.0959(0.1172) \n",
            "EVAL: [61/62] Elapsed 1m 10s (remain 0m 0s) Loss: 0.1394(0.1172) \n",
            "Epoch: [2][0/366] Elapsed 0m 1s (remain 9m 16s) Loss: 0.0993(0.0993) Grad: 132664.5781  LR: 0.00001809  \n",
            "Epoch: [2][20/366] Elapsed 0m 43s (remain 11m 52s) Loss: 0.1007(0.1084) Grad: 105600.1641  LR: 0.00001788  \n",
            "Epoch: [2][40/366] Elapsed 1m 22s (remain 10m 52s) Loss: 0.0884(0.1115) Grad: 49259.2461  LR: 0.00001766  \n",
            "Epoch: [2][60/366] Elapsed 1m 52s (remain 9m 20s) Loss: 0.0680(0.1073) Grad: 108320.7578  LR: 0.00001744  \n",
            "Epoch: [2][80/366] Elapsed 2m 26s (remain 8m 35s) Loss: 0.0809(0.1044) Grad: 57301.0391  LR: 0.00001721  \n",
            "Epoch: [2][100/366] Elapsed 3m 4s (remain 8m 3s) Loss: 0.0765(0.1022) Grad: 109719.0078  LR: 0.00001696  \n",
            "Epoch: [2][120/366] Elapsed 3m 40s (remain 7m 26s) Loss: 0.0681(0.1022) Grad: 94761.5312  LR: 0.00001671  \n",
            "Epoch: [2][140/366] Elapsed 4m 17s (remain 6m 51s) Loss: 0.0658(0.1032) Grad: 120961.5547  LR: 0.00001646  \n",
            "Epoch: [2][160/366] Elapsed 5m 4s (remain 6m 27s) Loss: 0.1188(0.1033) Grad: 102115.8281  LR: 0.00001619  \n",
            "Epoch: [2][180/366] Elapsed 5m 39s (remain 5m 47s) Loss: 0.1189(0.1032) Grad: 100852.7578  LR: 0.00001592  \n",
            "Epoch: [2][200/366] Elapsed 6m 17s (remain 5m 9s) Loss: 0.0922(0.1035) Grad: 93512.7188  LR: 0.00001564  \n",
            "Epoch: [2][220/366] Elapsed 6m 55s (remain 4m 32s) Loss: 0.0806(0.1028) Grad: 83646.5156  LR: 0.00001535  \n",
            "Epoch: [2][240/366] Elapsed 7m 32s (remain 3m 54s) Loss: 0.0940(0.1029) Grad: 98250.0469  LR: 0.00001506  \n",
            "Epoch: [2][260/366] Elapsed 8m 7s (remain 3m 16s) Loss: 0.0674(0.1021) Grad: 62912.4414  LR: 0.00001476  \n",
            "Epoch: [2][280/366] Elapsed 8m 46s (remain 2m 39s) Loss: 0.1371(0.1027) Grad: 85180.9453  LR: 0.00001446  \n",
            "Epoch: [2][300/366] Elapsed 9m 17s (remain 2m 0s) Loss: 0.0878(0.1033) Grad: 95834.1875  LR: 0.00001415  \n",
            "Epoch: [2][320/366] Elapsed 9m 51s (remain 1m 22s) Loss: 0.0838(0.1035) Grad: 83489.1406  LR: 0.00001383  \n",
            "Epoch: [2][340/366] Elapsed 10m 27s (remain 0m 45s) Loss: 0.0917(0.1033) Grad: 61327.5234  LR: 0.00001351  \n",
            "Epoch: [2][360/366] Elapsed 11m 1s (remain 0m 9s) Loss: 0.1290(0.1029) Grad: 129878.8203  LR: 0.00001319  \n",
            "Epoch: [2][365/366] Elapsed 11m 11s (remain 0m 0s) Loss: 0.1460(0.1031) Grad: 84708.9766  LR: 0.00001311  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 14s) Loss: 0.0785(0.0785) \n",
            "EVAL: [20/62] Elapsed 0m 23s (remain 0m 45s) Loss: 0.1009(0.1118) \n",
            "EVAL: [40/62] Elapsed 0m 47s (remain 0m 24s) Loss: 0.1226(0.1106) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1031  avg_val_loss: 0.1122  time: 741s\n",
            "Epoch 2 - Score: 0.4748  Scores: [0.518505217112889, 0.44943228475971403, 0.4412834160887432, 0.4675421449245012, 0.48459885522931184, 0.48719429333924563]\n",
            "Epoch 2 - Save Best Score: 0.4748 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.1032(0.1120) \n",
            "EVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.3117(0.1122) \n",
            "Epoch: [3][0/366] Elapsed 0m 4s (remain 24m 24s) Loss: 0.0724(0.0724) Grad: 146796.7969  LR: 0.00001309  \n",
            "Epoch: [3][20/366] Elapsed 0m 37s (remain 10m 19s) Loss: 0.0901(0.0947) Grad: 205534.8281  LR: 0.00001277  \n",
            "Epoch: [3][40/366] Elapsed 1m 14s (remain 9m 51s) Loss: 0.1641(0.1011) Grad: 198201.6719  LR: 0.00001243  \n",
            "Epoch: [3][60/366] Elapsed 1m 50s (remain 9m 11s) Loss: 0.1044(0.0983) Grad: 228772.2500  LR: 0.00001210  \n",
            "Epoch: [3][80/366] Elapsed 2m 28s (remain 8m 41s) Loss: 0.0654(0.0954) Grad: 117233.8281  LR: 0.00001176  \n",
            "Epoch: [3][100/366] Elapsed 3m 2s (remain 7m 59s) Loss: 0.0863(0.0941) Grad: 136152.3594  LR: 0.00001143  \n",
            "Epoch: [3][120/366] Elapsed 3m 39s (remain 7m 23s) Loss: 0.1123(0.0957) Grad: 353823.6250  LR: 0.00001109  \n",
            "Epoch: [3][140/366] Elapsed 4m 15s (remain 6m 47s) Loss: 0.1469(0.0948) Grad: 308057.7500  LR: 0.00001074  \n",
            "Epoch: [3][160/366] Elapsed 4m 48s (remain 6m 7s) Loss: 0.0993(0.0941) Grad: 264757.9062  LR: 0.00001040  \n",
            "Epoch: [3][180/366] Elapsed 5m 30s (remain 5m 37s) Loss: 0.0827(0.0940) Grad: 173646.3125  LR: 0.00001006  \n",
            "Epoch: [3][200/366] Elapsed 6m 10s (remain 5m 3s) Loss: 0.1154(0.0939) Grad: 189936.0781  LR: 0.00000972  \n",
            "Epoch: [3][220/366] Elapsed 6m 45s (remain 4m 26s) Loss: 0.0859(0.0939) Grad: 230295.4219  LR: 0.00000937  \n",
            "Epoch: [3][240/366] Elapsed 7m 19s (remain 3m 47s) Loss: 0.0965(0.0933) Grad: 240855.3594  LR: 0.00000903  \n",
            "Epoch: [3][260/366] Elapsed 7m 57s (remain 3m 12s) Loss: 0.1165(0.0935) Grad: 189828.5469  LR: 0.00000869  \n",
            "Epoch: [3][280/366] Elapsed 8m 32s (remain 2m 34s) Loss: 0.0855(0.0937) Grad: 154294.3438  LR: 0.00000835  \n",
            "Epoch: [3][300/366] Elapsed 9m 4s (remain 1m 57s) Loss: 0.0629(0.0937) Grad: 77767.2812  LR: 0.00000802  \n",
            "Epoch: [3][320/366] Elapsed 9m 44s (remain 1m 21s) Loss: 0.1107(0.0936) Grad: 152301.2812  LR: 0.00000768  \n",
            "Epoch: [3][340/366] Elapsed 10m 23s (remain 0m 45s) Loss: 0.0845(0.0932) Grad: 312940.6250  LR: 0.00000735  \n",
            "Epoch: [3][360/366] Elapsed 10m 56s (remain 0m 9s) Loss: 0.0962(0.0932) Grad: 146157.1875  LR: 0.00000702  \n",
            "Epoch: [3][365/366] Elapsed 11m 7s (remain 0m 0s) Loss: 0.1178(0.0932) Grad: 179869.8438  LR: 0.00000694  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 14s) Loss: 0.0811(0.0811) \n",
            "EVAL: [20/62] Elapsed 0m 23s (remain 0m 45s) Loss: 0.0886(0.1039) \n",
            "EVAL: [40/62] Elapsed 0m 47s (remain 0m 24s) Loss: 0.1059(0.1007) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0932  avg_val_loss: 0.1026  time: 737s\n",
            "Epoch 3 - Score: 0.4535  Scores: [0.48117085292346845, 0.44260119924765545, 0.40730468448399076, 0.45160246969464946, 0.48049994252348677, 0.45778602639632077]\n",
            "Epoch 3 - Save Best Score: 0.4535 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.1025(0.1025) \n",
            "EVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.1667(0.1026) \n",
            "Epoch: [4][0/366] Elapsed 0m 2s (remain 12m 50s) Loss: 0.1310(0.1310) Grad: 241771.5156  LR: 0.00000692  \n",
            "Epoch: [4][20/366] Elapsed 0m 41s (remain 11m 23s) Loss: 0.0932(0.0920) Grad: 175292.1875  LR: 0.00000660  \n",
            "Epoch: [4][40/366] Elapsed 1m 16s (remain 10m 3s) Loss: 0.0756(0.0874) Grad: 190115.8750  LR: 0.00000628  \n",
            "Epoch: [4][60/366] Elapsed 1m 49s (remain 9m 8s) Loss: 0.0653(0.0880) Grad: 150851.5781  LR: 0.00000596  \n",
            "Epoch: [4][80/366] Elapsed 2m 30s (remain 8m 49s) Loss: 0.0618(0.0877) Grad: 93135.4688  LR: 0.00000565  \n",
            "Epoch: [4][100/366] Elapsed 3m 4s (remain 8m 4s) Loss: 0.1111(0.0900) Grad: 122823.4453  LR: 0.00000535  \n",
            "Epoch: [4][120/366] Elapsed 3m 46s (remain 7m 38s) Loss: 0.0875(0.0898) Grad: 130664.5625  LR: 0.00000504  \n",
            "Epoch: [4][140/366] Elapsed 4m 18s (remain 6m 51s) Loss: 0.0651(0.0890) Grad: 157491.5781  LR: 0.00000475  \n",
            "Epoch: [4][160/366] Elapsed 4m 53s (remain 6m 13s) Loss: 0.0980(0.0882) Grad: 107896.0938  LR: 0.00000446  \n",
            "Epoch: [4][180/366] Elapsed 5m 29s (remain 5m 37s) Loss: 0.1061(0.0881) Grad: 225041.1406  LR: 0.00000418  \n",
            "Epoch: [4][200/366] Elapsed 6m 13s (remain 5m 6s) Loss: 0.0763(0.0874) Grad: 161858.1406  LR: 0.00000390  \n",
            "Epoch: [4][220/366] Elapsed 6m 46s (remain 4m 26s) Loss: 0.0875(0.0883) Grad: 113933.2500  LR: 0.00000364  \n",
            "Epoch: [4][240/366] Elapsed 7m 17s (remain 3m 46s) Loss: 0.0834(0.0884) Grad: 132593.1094  LR: 0.00000338  \n",
            "Epoch: [4][260/366] Elapsed 7m 55s (remain 3m 11s) Loss: 0.0654(0.0886) Grad: 128847.1953  LR: 0.00000312  \n",
            "Epoch: [4][280/366] Elapsed 8m 28s (remain 2m 33s) Loss: 0.0951(0.0883) Grad: 118036.5000  LR: 0.00000288  \n",
            "Epoch: [4][300/366] Elapsed 9m 6s (remain 1m 57s) Loss: 0.1334(0.0880) Grad: 148985.3125  LR: 0.00000264  \n",
            "Epoch: [4][320/366] Elapsed 9m 45s (remain 1m 22s) Loss: 0.0835(0.0879) Grad: 85662.2422  LR: 0.00000241  \n",
            "Epoch: [4][340/366] Elapsed 10m 18s (remain 0m 45s) Loss: 0.1180(0.0879) Grad: 286367.7500  LR: 0.00000219  \n",
            "Epoch: [4][360/366] Elapsed 11m 1s (remain 0m 9s) Loss: 0.1007(0.0876) Grad: 184482.9688  LR: 0.00000199  \n",
            "Epoch: [4][365/366] Elapsed 11m 8s (remain 0m 0s) Loss: 0.0608(0.0875) Grad: 89502.0938  LR: 0.00000193  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 17s) Loss: 0.0900(0.0900) \n",
            "EVAL: [20/62] Elapsed 0m 23s (remain 0m 45s) Loss: 0.0884(0.1073) \n",
            "EVAL: [40/62] Elapsed 0m 47s (remain 0m 24s) Loss: 0.1035(0.1031) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0875  avg_val_loss: 0.1054  time: 738s\n",
            "Epoch 4 - Score: 0.4600  Scores: [0.4930611698468935, 0.4438371756865151, 0.40814314134460855, 0.46035874571361546, 0.4876696564448261, 0.4668620834309125]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.1051(0.1054) \n",
            "EVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.1193(0.1054) \n",
            "Epoch: [5][0/366] Elapsed 0m 2s (remain 15m 19s) Loss: 0.0915(0.0915) Grad: 211130.7188  LR: 0.00000192  \n",
            "Epoch: [5][20/366] Elapsed 0m 30s (remain 8m 28s) Loss: 0.0943(0.0835) Grad: 214553.3125  LR: 0.00000173  \n",
            "Epoch: [5][40/366] Elapsed 1m 11s (remain 9m 26s) Loss: 0.0784(0.0871) Grad: 200855.9531  LR: 0.00000154  \n",
            "Epoch: [5][60/366] Elapsed 1m 50s (remain 9m 10s) Loss: 0.0721(0.0872) Grad: 157207.3281  LR: 0.00000136  \n",
            "Epoch: [5][80/366] Elapsed 2m 28s (remain 8m 43s) Loss: 0.0822(0.0873) Grad: 172869.8125  LR: 0.00000119  \n",
            "Epoch: [5][100/366] Elapsed 3m 2s (remain 7m 59s) Loss: 0.0680(0.0871) Grad: 126036.0391  LR: 0.00000104  \n",
            "Epoch: [5][120/366] Elapsed 3m 40s (remain 7m 26s) Loss: 0.0623(0.0853) Grad: 96597.7812  LR: 0.00000089  \n",
            "Epoch: [5][140/366] Elapsed 4m 12s (remain 6m 42s) Loss: 0.1045(0.0845) Grad: 149400.2969  LR: 0.00000075  \n",
            "Epoch: [5][160/366] Elapsed 4m 47s (remain 6m 5s) Loss: 0.0845(0.0843) Grad: 125075.8594  LR: 0.00000063  \n",
            "Epoch: [5][180/366] Elapsed 5m 21s (remain 5m 28s) Loss: 0.0593(0.0846) Grad: 97639.8125  LR: 0.00000051  \n",
            "Epoch: [5][200/366] Elapsed 5m 56s (remain 4m 53s) Loss: 0.0663(0.0846) Grad: 205633.2188  LR: 0.00000041  \n",
            "Epoch: [5][220/366] Elapsed 6m 31s (remain 4m 16s) Loss: 0.1090(0.0843) Grad: 161829.7656  LR: 0.00000032  \n",
            "Epoch: [5][240/366] Elapsed 7m 11s (remain 3m 43s) Loss: 0.0678(0.0845) Grad: 99049.0078  LR: 0.00000024  \n",
            "Epoch: [5][260/366] Elapsed 7m 48s (remain 3m 8s) Loss: 0.0943(0.0843) Grad: 188429.1562  LR: 0.00000017  \n",
            "Epoch: [5][280/366] Elapsed 8m 25s (remain 2m 32s) Loss: 0.0531(0.0845) Grad: 124357.7344  LR: 0.00000011  \n",
            "Epoch: [5][300/366] Elapsed 9m 3s (remain 1m 57s) Loss: 0.0846(0.0847) Grad: 158952.5938  LR: 0.00000007  \n",
            "Epoch: [5][320/366] Elapsed 9m 42s (remain 1m 21s) Loss: 0.0819(0.0844) Grad: 126680.5703  LR: 0.00000003  \n",
            "Epoch: [5][340/366] Elapsed 10m 20s (remain 0m 45s) Loss: 0.0899(0.0845) Grad: 155056.4844  LR: 0.00000001  \n",
            "Epoch: [5][360/366] Elapsed 10m 59s (remain 0m 9s) Loss: 0.0679(0.0847) Grad: 142540.4219  LR: 0.00000000  \n",
            "Epoch: [5][365/366] Elapsed 11m 9s (remain 0m 0s) Loss: 0.0841(0.0846) Grad: 91586.8125  LR: 0.00000000  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 14s) Loss: 0.0852(0.0852) \n",
            "EVAL: [20/62] Elapsed 0m 23s (remain 0m 45s) Loss: 0.0879(0.1046) \n",
            "EVAL: [40/62] Elapsed 0m 47s (remain 0m 24s) Loss: 0.1038(0.1006) \n",
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.1039(0.1028) \n",
            "EVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.1335(0.1028) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0846  avg_val_loss: 0.1028  time: 740s\n",
            "Epoch 5 - Score: 0.4541  Scores: [0.48104976580720415, 0.44073006876148846, 0.4084444241003377, 0.45126805628252614, 0.481057429026221, 0.4619987114760102]\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.4535  Scores: [0.48117085292346845, 0.44260119924765545, 0.40730468448399076, 0.45160246969464946, 0.48049994252348677, 0.45778602639632077]\n",
            "========== fold: 1 training ==========\n",
            "DebertaV2Config {\n",
            "  \"_name_or_path\": \"/kaggle/input/debertav3base/\",\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at /kaggle/input/debertav3base/ were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/366] Elapsed 0m 2s (remain 15m 45s) Loss: 2.6186(2.6186) Grad: inf  LR: 0.00002000  \n",
            "Epoch: [1][20/366] Elapsed 0m 40s (remain 11m 4s) Loss: 0.1078(1.3688) Grad: 70585.6953  LR: 0.00001999  \n",
            "Epoch: [1][40/366] Elapsed 1m 14s (remain 9m 51s) Loss: 0.1497(0.7886) Grad: 127961.9297  LR: 0.00001998  \n",
            "Epoch: [1][60/366] Elapsed 1m 52s (remain 9m 22s) Loss: 0.1642(0.5829) Grad: 60625.3242  LR: 0.00001995  \n",
            "Epoch: [1][80/366] Elapsed 2m 30s (remain 8m 49s) Loss: 0.1139(0.4770) Grad: 49952.0391  LR: 0.00001990  \n",
            "Epoch: [1][100/366] Elapsed 3m 3s (remain 8m 0s) Loss: 0.1660(0.4106) Grad: 198367.7656  LR: 0.00001985  \n",
            "Epoch: [1][120/366] Elapsed 3m 46s (remain 7m 38s) Loss: 0.2348(0.3670) Grad: 178132.9062  LR: 0.00001979  \n",
            "Epoch: [1][140/366] Elapsed 4m 22s (remain 6m 58s) Loss: 0.1111(0.3334) Grad: 72750.3594  LR: 0.00001971  \n",
            "Epoch: [1][160/366] Elapsed 5m 0s (remain 6m 22s) Loss: 0.1064(0.3071) Grad: 104039.9141  LR: 0.00001962  \n",
            "Epoch: [1][180/366] Elapsed 5m 33s (remain 5m 41s) Loss: 0.1055(0.2861) Grad: 58852.2109  LR: 0.00001952  \n",
            "Epoch: [1][200/366] Elapsed 6m 11s (remain 5m 4s) Loss: 0.1104(0.2746) Grad: 45615.9844  LR: 0.00001941  \n",
            "Epoch: [1][220/366] Elapsed 6m 47s (remain 4m 27s) Loss: 0.1645(0.2637) Grad: 76212.6094  LR: 0.00001929  \n",
            "Epoch: [1][240/366] Elapsed 7m 26s (remain 3m 51s) Loss: 0.1958(0.2517) Grad: 101493.3672  LR: 0.00001916  \n",
            "Epoch: [1][260/366] Elapsed 8m 1s (remain 3m 13s) Loss: 0.1376(0.2414) Grad: 60685.6953  LR: 0.00001902  \n",
            "Epoch: [1][280/366] Elapsed 8m 37s (remain 2m 36s) Loss: 0.0899(0.2325) Grad: 63237.0586  LR: 0.00001886  \n",
            "Epoch: [1][300/366] Elapsed 9m 21s (remain 2m 1s) Loss: 0.1273(0.2260) Grad: 41156.3477  LR: 0.00001870  \n",
            "Epoch: [1][320/366] Elapsed 9m 57s (remain 1m 23s) Loss: 0.0944(0.2196) Grad: 41739.8906  LR: 0.00001852  \n",
            "Epoch: [1][340/366] Elapsed 10m 29s (remain 0m 46s) Loss: 0.0966(0.2135) Grad: 67352.0625  LR: 0.00001834  \n",
            "Epoch: [1][360/366] Elapsed 11m 1s (remain 0m 9s) Loss: 0.1390(0.2080) Grad: 67635.6719  LR: 0.00001815  \n",
            "Epoch: [1][365/366] Elapsed 11m 8s (remain 0m 0s) Loss: 0.1408(0.2067) Grad: 68807.9922  LR: 0.00001810  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 21s) Loss: 0.1130(0.1130) \n",
            "EVAL: [20/62] Elapsed 0m 28s (remain 0m 55s) Loss: 0.1036(0.1215) \n",
            "EVAL: [40/62] Elapsed 0m 50s (remain 0m 25s) Loss: 0.1273(0.1251) \n",
            "EVAL: [60/62] Elapsed 1m 10s (remain 0m 1s) Loss: 0.1091(0.1213) \n",
            "EVAL: [61/62] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0725(0.1212) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2067  avg_val_loss: 0.1212  time: 740s\n",
            "Epoch 1 - Score: 0.4925  Scores: [0.5950716692156336, 0.46834352305365384, 0.42985427299761353, 0.5003445408237472, 0.4875930877730655, 0.47361892520254006]\n",
            "Epoch 1 - Save Best Score: 0.4925 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/366] Elapsed 0m 1s (remain 9m 42s) Loss: 0.1290(0.1290) Grad: inf  LR: 0.00001809  \n",
            "Epoch: [2][20/366] Elapsed 0m 38s (remain 10m 36s) Loss: 0.1628(0.1114) Grad: 225714.2031  LR: 0.00001788  \n",
            "Epoch: [2][40/366] Elapsed 1m 16s (remain 10m 6s) Loss: 0.1559(0.1095) Grad: 184036.1719  LR: 0.00001766  \n",
            "Epoch: [2][60/366] Elapsed 1m 49s (remain 9m 7s) Loss: 0.1133(0.1090) Grad: 144973.7656  LR: 0.00001744  \n",
            "Epoch: [2][80/366] Elapsed 2m 30s (remain 8m 49s) Loss: 0.1828(0.1137) Grad: 143772.7500  LR: 0.00001721  \n",
            "Epoch: [2][100/366] Elapsed 3m 7s (remain 8m 12s) Loss: 0.1088(0.1104) Grad: 97082.3828  LR: 0.00001696  \n",
            "Epoch: [2][120/366] Elapsed 3m 42s (remain 7m 30s) Loss: 0.0991(0.1112) Grad: 71216.5234  LR: 0.00001671  \n",
            "Epoch: [2][140/366] Elapsed 4m 20s (remain 6m 54s) Loss: 0.1364(0.1111) Grad: 216348.1719  LR: 0.00001646  \n",
            "Epoch: [2][160/366] Elapsed 4m 57s (remain 6m 18s) Loss: 0.1137(0.1111) Grad: 146086.0625  LR: 0.00001619  \n",
            "Epoch: [2][180/366] Elapsed 5m 33s (remain 5m 41s) Loss: 0.0874(0.1105) Grad: 113801.3438  LR: 0.00001592  \n",
            "Epoch: [2][200/366] Elapsed 6m 13s (remain 5m 6s) Loss: 0.1063(0.1108) Grad: 100328.5391  LR: 0.00001564  \n",
            "Epoch: [2][220/366] Elapsed 6m 53s (remain 4m 31s) Loss: 0.1169(0.1097) Grad: 80538.3984  LR: 0.00001535  \n",
            "Epoch: [2][240/366] Elapsed 7m 30s (remain 3m 53s) Loss: 0.2459(0.1090) Grad: 88750.8359  LR: 0.00001506  \n",
            "Epoch: [2][260/366] Elapsed 7m 59s (remain 3m 12s) Loss: 0.1014(0.1081) Grad: 58374.3008  LR: 0.00001476  \n",
            "Epoch: [2][280/366] Elapsed 8m 35s (remain 2m 35s) Loss: 0.1403(0.1077) Grad: 184194.3750  LR: 0.00001446  \n",
            "Epoch: [2][300/366] Elapsed 9m 15s (remain 2m 0s) Loss: 0.0711(0.1072) Grad: 95572.1016  LR: 0.00001415  \n",
            "Epoch: [2][320/366] Elapsed 9m 51s (remain 1m 22s) Loss: 0.1451(0.1071) Grad: 163406.7031  LR: 0.00001383  \n",
            "Epoch: [2][340/366] Elapsed 10m 29s (remain 0m 46s) Loss: 0.1188(0.1068) Grad: 79130.7344  LR: 0.00001351  \n",
            "Epoch: [2][360/366] Elapsed 11m 4s (remain 0m 9s) Loss: 0.1082(0.1063) Grad: 139507.4375  LR: 0.00001319  \n",
            "Epoch: [2][365/366] Elapsed 11m 13s (remain 0m 0s) Loss: 0.0950(0.1061) Grad: 99629.1953  LR: 0.00001311  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 10s) Loss: 0.1068(0.1068) \n",
            "EVAL: [20/62] Elapsed 0m 27s (remain 0m 53s) Loss: 0.0896(0.1065) \n",
            "EVAL: [40/62] Elapsed 0m 49s (remain 0m 25s) Loss: 0.1185(0.1109) \n",
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0983(0.1104) \n",
            "EVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0739(0.1103) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1061  avg_val_loss: 0.1103  time: 744s\n",
            "Epoch 2 - Score: 0.4710  Scores: [0.5063506726500945, 0.44864769570711227, 0.4479158418230321, 0.4761561334626112, 0.4956693222657946, 0.4509865635569898]\n",
            "Epoch 2 - Save Best Score: 0.4710 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [3][0/366] Elapsed 0m 1s (remain 11m 55s) Loss: 0.0901(0.0901) Grad: 243087.0312  LR: 0.00001309  \n",
            "Epoch: [3][20/366] Elapsed 0m 42s (remain 11m 30s) Loss: 0.1048(0.0949) Grad: 249317.8750  LR: 0.00001277  \n",
            "Epoch: [3][40/366] Elapsed 1m 16s (remain 10m 3s) Loss: 0.1124(0.0928) Grad: 210587.7812  LR: 0.00001243  \n",
            "Epoch: [3][60/366] Elapsed 1m 49s (remain 9m 8s) Loss: 0.0316(0.0895) Grad: 135745.7969  LR: 0.00001210  \n",
            "Epoch: [3][80/366] Elapsed 2m 28s (remain 8m 41s) Loss: 0.0763(0.0904) Grad: 156789.4219  LR: 0.00001176  \n",
            "Epoch: [3][100/366] Elapsed 3m 1s (remain 7m 56s) Loss: 0.0780(0.0919) Grad: 193278.4844  LR: 0.00001143  \n",
            "Epoch: [3][120/366] Elapsed 3m 39s (remain 7m 23s) Loss: 0.1172(0.0925) Grad: 80283.8906  LR: 0.00001109  \n",
            "Epoch: [3][140/366] Elapsed 4m 11s (remain 6m 40s) Loss: 0.1601(0.0927) Grad: 86895.2734  LR: 0.00001074  \n",
            "Epoch: [3][160/366] Elapsed 4m 47s (remain 6m 6s) Loss: 0.1325(0.0943) Grad: 177370.5781  LR: 0.00001040  \n",
            "Epoch: [3][180/366] Elapsed 5m 23s (remain 5m 31s) Loss: 0.0792(0.0942) Grad: 121665.6484  LR: 0.00001006  \n",
            "Epoch: [3][200/366] Elapsed 5m 59s (remain 4m 55s) Loss: 0.0982(0.0943) Grad: 102891.9219  LR: 0.00000972  \n",
            "Epoch: [3][220/366] Elapsed 6m 42s (remain 4m 24s) Loss: 0.1167(0.0951) Grad: 116348.2344  LR: 0.00000937  \n",
            "Epoch: [3][240/366] Elapsed 7m 19s (remain 3m 47s) Loss: 0.1040(0.0952) Grad: 64481.7383  LR: 0.00000903  \n",
            "Epoch: [3][260/366] Elapsed 7m 59s (remain 3m 12s) Loss: 0.0786(0.0953) Grad: 54081.8789  LR: 0.00000869  \n",
            "Epoch: [3][280/366] Elapsed 8m 37s (remain 2m 36s) Loss: 0.0783(0.0956) Grad: 77277.5938  LR: 0.00000835  \n",
            "Epoch: [3][300/366] Elapsed 9m 13s (remain 1m 59s) Loss: 0.0821(0.0957) Grad: 59226.4883  LR: 0.00000802  \n",
            "Epoch: [3][320/366] Elapsed 9m 48s (remain 1m 22s) Loss: 0.0689(0.0954) Grad: 104663.6094  LR: 0.00000768  \n",
            "Epoch: [3][340/366] Elapsed 10m 21s (remain 0m 45s) Loss: 0.1140(0.0959) Grad: 89472.6172  LR: 0.00000735  \n",
            "Epoch: [3][360/366] Elapsed 10m 55s (remain 0m 9s) Loss: 0.1973(0.0958) Grad: 239030.1875  LR: 0.00000702  \n",
            "Epoch: [3][365/366] Elapsed 11m 7s (remain 0m 0s) Loss: 0.0782(0.0957) Grad: 113803.1875  LR: 0.00000694  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 11s) Loss: 0.1167(0.1167) \n",
            "EVAL: [20/62] Elapsed 0m 27s (remain 0m 54s) Loss: 0.0898(0.1081) \n",
            "EVAL: [40/62] Elapsed 0m 49s (remain 0m 25s) Loss: 0.1096(0.1116) \n",
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0974(0.1096) \n",
            "EVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0841(0.1096) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0957  avg_val_loss: 0.1096  time: 738s\n",
            "Epoch 3 - Score: 0.4694  Scores: [0.5075549153818055, 0.4630397613660995, 0.42907840327263336, 0.4684789961711547, 0.4864373348060382, 0.46177043023965214]\n",
            "Epoch 3 - Save Best Score: 0.4694 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [4][0/366] Elapsed 0m 2s (remain 15m 17s) Loss: 0.1174(0.1174) Grad: 132398.0781  LR: 0.00000692  \n",
            "Epoch: [4][20/366] Elapsed 0m 40s (remain 10m 59s) Loss: 0.0880(0.0868) Grad: 222821.0781  LR: 0.00000660  \n",
            "Epoch: [4][40/366] Elapsed 1m 10s (remain 9m 21s) Loss: 0.0528(0.0896) Grad: 74502.5703  LR: 0.00000628  \n",
            "Epoch: [4][60/366] Elapsed 1m 53s (remain 9m 27s) Loss: 0.0965(0.0926) Grad: 75628.3203  LR: 0.00000596  \n",
            "Epoch: [4][80/366] Elapsed 2m 37s (remain 9m 14s) Loss: 0.0728(0.0913) Grad: 75084.0000  LR: 0.00000565  \n",
            "Epoch: [4][100/366] Elapsed 3m 17s (remain 8m 37s) Loss: 0.0736(0.0906) Grad: 110710.2109  LR: 0.00000535  \n",
            "Epoch: [4][120/366] Elapsed 3m 49s (remain 7m 44s) Loss: 0.1194(0.0918) Grad: 83585.7812  LR: 0.00000504  \n",
            "Epoch: [4][140/366] Elapsed 4m 25s (remain 7m 4s) Loss: 0.0839(0.0903) Grad: 119024.8984  LR: 0.00000475  \n",
            "Epoch: [4][160/366] Elapsed 4m 59s (remain 6m 21s) Loss: 0.0911(0.0903) Grad: 122904.0625  LR: 0.00000446  \n",
            "Epoch: [4][180/366] Elapsed 5m 33s (remain 5m 40s) Loss: 0.0984(0.0898) Grad: 74521.7656  LR: 0.00000418  \n",
            "Epoch: [4][200/366] Elapsed 6m 9s (remain 5m 3s) Loss: 0.1050(0.0901) Grad: 148358.1094  LR: 0.00000390  \n",
            "Epoch: [4][220/366] Elapsed 6m 42s (remain 4m 23s) Loss: 0.0977(0.0902) Grad: 106249.1797  LR: 0.00000364  \n",
            "Epoch: [4][240/366] Elapsed 7m 17s (remain 3m 46s) Loss: 0.0650(0.0904) Grad: 131813.2188  LR: 0.00000338  \n",
            "Epoch: [4][260/366] Elapsed 7m 53s (remain 3m 10s) Loss: 0.1219(0.0903) Grad: 115789.6250  LR: 0.00000312  \n",
            "Epoch: [4][280/366] Elapsed 8m 26s (remain 2m 33s) Loss: 0.1083(0.0895) Grad: 103535.2344  LR: 0.00000288  \n",
            "Epoch: [4][300/366] Elapsed 9m 6s (remain 1m 57s) Loss: 0.0942(0.0892) Grad: 121480.2109  LR: 0.00000264  \n",
            "Epoch: [4][320/366] Elapsed 9m 41s (remain 1m 21s) Loss: 0.0818(0.0894) Grad: 133353.0781  LR: 0.00000241  \n",
            "Epoch: [4][340/366] Elapsed 10m 20s (remain 0m 45s) Loss: 0.0832(0.0889) Grad: 95083.3594  LR: 0.00000219  \n",
            "Epoch: [4][360/366] Elapsed 10m 58s (remain 0m 9s) Loss: 0.0686(0.0895) Grad: 119276.3281  LR: 0.00000199  \n",
            "Epoch: [4][365/366] Elapsed 11m 7s (remain 0m 0s) Loss: 0.0635(0.0893) Grad: 103949.5391  LR: 0.00000193  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 15s) Loss: 0.1119(0.1119) \n",
            "EVAL: [20/62] Elapsed 0m 27s (remain 0m 54s) Loss: 0.0874(0.1044) \n",
            "EVAL: [40/62] Elapsed 0m 49s (remain 0m 25s) Loss: 0.1106(0.1081) \n",
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0971(0.1078) \n",
            "EVAL: [61/62] Elapsed 1m 10s (remain 0m 0s) Loss: 0.1106(0.1078) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0893  avg_val_loss: 0.1078  time: 737s\n",
            "Epoch 4 - Score: 0.4654  Scores: [0.5067774834391483, 0.45344570921870164, 0.4292903845739587, 0.47269260543314656, 0.4807932095865946, 0.44956432662885254]\n",
            "Epoch 4 - Save Best Score: 0.4654 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [5][0/366] Elapsed 0m 3s (remain 20m 18s) Loss: 0.0738(0.0738) Grad: 201444.0625  LR: 0.00000192  \n",
            "Epoch: [5][20/366] Elapsed 0m 41s (remain 11m 18s) Loss: 0.0689(0.0875) Grad: 240021.3438  LR: 0.00000173  \n",
            "Epoch: [5][40/366] Elapsed 1m 21s (remain 10m 43s) Loss: 0.0670(0.0843) Grad: 109692.1719  LR: 0.00000154  \n",
            "Epoch: [5][60/366] Elapsed 1m 57s (remain 9m 49s) Loss: 0.0966(0.0852) Grad: 184033.2031  LR: 0.00000136  \n",
            "Epoch: [5][80/366] Elapsed 2m 37s (remain 9m 13s) Loss: 0.0761(0.0853) Grad: 99488.9844  LR: 0.00000119  \n",
            "Epoch: [5][100/366] Elapsed 3m 13s (remain 8m 26s) Loss: 0.1189(0.0863) Grad: 61058.0039  LR: 0.00000104  \n",
            "Epoch: [5][120/366] Elapsed 3m 40s (remain 7m 26s) Loss: 0.0704(0.0858) Grad: 75297.6094  LR: 0.00000089  \n",
            "Epoch: [5][140/366] Elapsed 4m 17s (remain 6m 50s) Loss: 0.0584(0.0848) Grad: 61393.8789  LR: 0.00000075  \n",
            "Epoch: [5][160/366] Elapsed 4m 50s (remain 6m 10s) Loss: 0.0621(0.0845) Grad: 56876.7500  LR: 0.00000063  \n",
            "Epoch: [5][180/366] Elapsed 5m 27s (remain 5m 34s) Loss: 0.1000(0.0841) Grad: 100862.4609  LR: 0.00000051  \n",
            "Epoch: [5][200/366] Elapsed 6m 6s (remain 5m 0s) Loss: 0.1000(0.0850) Grad: 134979.9375  LR: 0.00000041  \n",
            "Epoch: [5][220/366] Elapsed 6m 42s (remain 4m 24s) Loss: 0.0569(0.0842) Grad: 56914.2656  LR: 0.00000032  \n",
            "Epoch: [5][240/366] Elapsed 7m 19s (remain 3m 48s) Loss: 0.0720(0.0847) Grad: 67635.7109  LR: 0.00000024  \n",
            "Epoch: [5][260/366] Elapsed 7m 50s (remain 3m 9s) Loss: 0.1049(0.0846) Grad: 122726.0000  LR: 0.00000017  \n",
            "Epoch: [5][280/366] Elapsed 8m 25s (remain 2m 33s) Loss: 0.0805(0.0846) Grad: 147501.0469  LR: 0.00000011  \n",
            "Epoch: [5][300/366] Elapsed 8m 57s (remain 1m 56s) Loss: 0.0479(0.0841) Grad: 52563.5078  LR: 0.00000007  \n",
            "Epoch: [5][320/366] Elapsed 9m 35s (remain 1m 20s) Loss: 0.0959(0.0843) Grad: 157891.2812  LR: 0.00000003  \n",
            "Epoch: [5][340/366] Elapsed 10m 11s (remain 0m 44s) Loss: 0.1169(0.0849) Grad: 159715.4375  LR: 0.00000001  \n",
            "Epoch: [5][360/366] Elapsed 10m 53s (remain 0m 9s) Loss: 0.0857(0.0846) Grad: 78075.1250  LR: 0.00000000  \n",
            "Epoch: [5][365/366] Elapsed 11m 2s (remain 0m 0s) Loss: 0.0651(0.0845) Grad: 97946.0703  LR: 0.00000000  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 11s) Loss: 0.1143(0.1143) \n",
            "EVAL: [20/62] Elapsed 0m 27s (remain 0m 54s) Loss: 0.0882(0.1046) \n",
            "EVAL: [40/62] Elapsed 0m 49s (remain 0m 25s) Loss: 0.1098(0.1079) \n",
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0961(0.1073) \n",
            "EVAL: [61/62] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0969(0.1073) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0845  avg_val_loss: 0.1073  time: 733s\n",
            "Epoch 5 - Score: 0.4643  Scores: [0.5033233474032056, 0.4517743618555293, 0.428225181043949, 0.47143894696864036, 0.4820258599401173, 0.4488555485638754]\n",
            "Epoch 5 - Save Best Score: 0.4643 Model\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.4643  Scores: [0.5033233474032056, 0.4517743618555293, 0.428225181043949, 0.47143894696864036, 0.4820258599401173, 0.4488555485638754]\n",
            "========== fold: 2 training ==========\n",
            "DebertaV2Config {\n",
            "  \"_name_or_path\": \"/kaggle/input/debertav3base/\",\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at /kaggle/input/debertav3base/ were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/366] Elapsed 0m 2s (remain 14m 22s) Loss: 2.1853(2.1853) Grad: inf  LR: 0.00002000  \n",
            "Epoch: [1][20/366] Elapsed 0m 40s (remain 10m 57s) Loss: 0.2192(1.1833) Grad: 126762.8594  LR: 0.00001999  \n",
            "Epoch: [1][40/366] Elapsed 1m 16s (remain 10m 6s) Loss: 0.3823(0.7148) Grad: 186256.7969  LR: 0.00001998  \n",
            "Epoch: [1][60/366] Elapsed 1m 52s (remain 9m 24s) Loss: 0.2492(0.5455) Grad: 193189.8438  LR: 0.00001995  \n",
            "Epoch: [1][80/366] Elapsed 2m 33s (remain 8m 59s) Loss: 0.1332(0.4476) Grad: 77358.5859  LR: 0.00001990  \n",
            "Epoch: [1][100/366] Elapsed 3m 7s (remain 8m 11s) Loss: 0.1214(0.3868) Grad: 117346.1484  LR: 0.00001985  \n",
            "Epoch: [1][120/366] Elapsed 3m 50s (remain 7m 46s) Loss: 0.1222(0.3447) Grad: 57240.1250  LR: 0.00001979  \n",
            "Epoch: [1][140/366] Elapsed 4m 25s (remain 7m 2s) Loss: 0.1489(0.3131) Grad: 117089.4375  LR: 0.00001971  \n",
            "Epoch: [1][160/366] Elapsed 4m 58s (remain 6m 19s) Loss: 0.1628(0.2916) Grad: 150434.7500  LR: 0.00001962  \n",
            "Epoch: [1][180/366] Elapsed 5m 29s (remain 5m 36s) Loss: 0.1452(0.2739) Grad: 134720.2344  LR: 0.00001952  \n",
            "Epoch: [1][200/366] Elapsed 6m 13s (remain 5m 6s) Loss: 0.1207(0.2591) Grad: 63915.9609  LR: 0.00001941  \n",
            "Epoch: [1][220/366] Elapsed 6m 45s (remain 4m 25s) Loss: 0.1656(0.2488) Grad: 66011.9766  LR: 0.00001929  \n",
            "Epoch: [1][240/366] Elapsed 7m 17s (remain 3m 46s) Loss: 0.1124(0.2380) Grad: 76252.4688  LR: 0.00001916  \n",
            "Epoch: [1][260/366] Elapsed 7m 54s (remain 3m 10s) Loss: 0.0647(0.2290) Grad: 30288.1484  LR: 0.00001902  \n",
            "Epoch: [1][280/366] Elapsed 8m 35s (remain 2m 35s) Loss: 0.1168(0.2216) Grad: 65813.0391  LR: 0.00001886  \n",
            "Epoch: [1][300/366] Elapsed 9m 12s (remain 1m 59s) Loss: 0.1137(0.2147) Grad: 54183.9062  LR: 0.00001870  \n",
            "Epoch: [1][320/366] Elapsed 9m 46s (remain 1m 22s) Loss: 0.1443(0.2096) Grad: 106042.6172  LR: 0.00001852  \n",
            "Epoch: [1][340/366] Elapsed 10m 23s (remain 0m 45s) Loss: 0.1039(0.2043) Grad: 71508.4531  LR: 0.00001834  \n",
            "Epoch: [1][360/366] Elapsed 11m 2s (remain 0m 9s) Loss: 0.0742(0.2000) Grad: 23040.5664  LR: 0.00001815  \n",
            "Epoch: [1][365/366] Elapsed 11m 13s (remain 0m 0s) Loss: 0.1975(0.1990) Grad: 136865.5469  LR: 0.00001810  \n",
            "EVAL: [0/62] Elapsed 0m 0s (remain 0m 56s) Loss: 0.1826(0.1826) \n",
            "EVAL: [20/62] Elapsed 0m 22s (remain 0m 43s) Loss: 0.0883(0.1427) \n",
            "EVAL: [40/62] Elapsed 0m 45s (remain 0m 23s) Loss: 0.1423(0.1346) \n",
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0973(0.1325) \n",
            "EVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.1867(0.1326) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1990  avg_val_loss: 0.1326  time: 743s\n",
            "Epoch 1 - Score: 0.5183  Scores: [0.5227589217268973, 0.5270870068347155, 0.51705989920806, 0.5435950290674316, 0.5169029096645373, 0.48221947016704536]\n",
            "Epoch 1 - Save Best Score: 0.5183 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/366] Elapsed 0m 2s (remain 17m 33s) Loss: 0.1519(0.1519) Grad: inf  LR: 0.00001809  \n",
            "Epoch: [2][20/366] Elapsed 0m 39s (remain 10m 45s) Loss: 0.1144(0.1079) Grad: 135854.0000  LR: 0.00001788  \n",
            "Epoch: [2][40/366] Elapsed 1m 14s (remain 9m 54s) Loss: 0.1143(0.1069) Grad: 200769.7188  LR: 0.00001766  \n",
            "Epoch: [2][60/366] Elapsed 1m 49s (remain 9m 8s) Loss: 0.1214(0.1096) Grad: 153768.4375  LR: 0.00001744  \n",
            "Epoch: [2][80/366] Elapsed 2m 27s (remain 8m 37s) Loss: 0.1472(0.1084) Grad: 149613.8438  LR: 0.00001721  \n",
            "Epoch: [2][100/366] Elapsed 3m 0s (remain 7m 52s) Loss: 0.1255(0.1067) Grad: 72942.4453  LR: 0.00001696  \n",
            "Epoch: [2][120/366] Elapsed 3m 38s (remain 7m 22s) Loss: 0.0993(0.1070) Grad: 158477.7812  LR: 0.00001671  \n",
            "Epoch: [2][140/366] Elapsed 4m 14s (remain 6m 46s) Loss: 0.1489(0.1079) Grad: 106016.4766  LR: 0.00001646  \n",
            "Epoch: [2][160/366] Elapsed 4m 50s (remain 6m 10s) Loss: 0.0562(0.1075) Grad: 126872.9141  LR: 0.00001619  \n",
            "Epoch: [2][180/366] Elapsed 5m 31s (remain 5m 38s) Loss: 0.0928(0.1090) Grad: 123847.7656  LR: 0.00001592  \n",
            "Epoch: [2][200/366] Elapsed 6m 9s (remain 5m 3s) Loss: 0.1353(0.1093) Grad: 125356.8984  LR: 0.00001564  \n",
            "Epoch: [2][220/366] Elapsed 6m 47s (remain 4m 27s) Loss: 0.1010(0.1093) Grad: 83853.4375  LR: 0.00001535  \n",
            "Epoch: [2][240/366] Elapsed 7m 26s (remain 3m 51s) Loss: 0.0996(0.1094) Grad: 138305.5781  LR: 0.00001506  \n",
            "Epoch: [2][260/366] Elapsed 8m 4s (remain 3m 14s) Loss: 0.1077(0.1087) Grad: 232189.0781  LR: 0.00001476  \n",
            "Epoch: [2][280/366] Elapsed 8m 43s (remain 2m 38s) Loss: 0.0678(0.1079) Grad: 114541.5391  LR: 0.00001446  \n",
            "Epoch: [2][300/366] Elapsed 9m 15s (remain 2m 0s) Loss: 0.0783(0.1072) Grad: 174802.9375  LR: 0.00001415  \n",
            "Epoch: [2][320/366] Elapsed 9m 49s (remain 1m 22s) Loss: 0.0724(0.1066) Grad: 80105.5781  LR: 0.00001383  \n",
            "Epoch: [2][340/366] Elapsed 10m 32s (remain 0m 46s) Loss: 0.1619(0.1071) Grad: 308768.5938  LR: 0.00001351  \n",
            "Epoch: [2][360/366] Elapsed 11m 4s (remain 0m 9s) Loss: 0.0909(0.1072) Grad: 133062.0781  LR: 0.00001319  \n",
            "Epoch: [2][365/366] Elapsed 11m 11s (remain 0m 0s) Loss: 0.1690(0.1069) Grad: 139493.4531  LR: 0.00001311  \n",
            "EVAL: [0/62] Elapsed 0m 0s (remain 0m 50s) Loss: 0.1459(0.1459) \n",
            "EVAL: [20/62] Elapsed 0m 21s (remain 0m 42s) Loss: 0.0790(0.1258) \n",
            "EVAL: [40/62] Elapsed 0m 44s (remain 0m 22s) Loss: 0.1224(0.1165) \n",
            "EVAL: [60/62] Elapsed 1m 8s (remain 0m 1s) Loss: 0.0872(0.1158) \n",
            "EVAL: [61/62] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1422(0.1159) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1069  avg_val_loss: 0.1159  time: 741s\n",
            "Epoch 2 - Score: 0.4828  Scores: [0.5041247069781106, 0.487657123107037, 0.4357074165377607, 0.4739384958248586, 0.5266730666650974, 0.4684065638340271]\n",
            "Epoch 2 - Save Best Score: 0.4828 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [3][0/366] Elapsed 0m 3s (remain 21m 24s) Loss: 0.1824(0.1824) Grad: 250902.8438  LR: 0.00001309  \n",
            "Epoch: [3][20/366] Elapsed 0m 37s (remain 10m 9s) Loss: 0.0878(0.1050) Grad: 108037.2734  LR: 0.00001277  \n",
            "Epoch: [3][40/366] Elapsed 1m 22s (remain 10m 51s) Loss: 0.0851(0.0986) Grad: 111627.6875  LR: 0.00001243  \n",
            "Epoch: [3][60/366] Elapsed 1m 59s (remain 9m 55s) Loss: 0.1309(0.1004) Grad: 125506.8750  LR: 0.00001210  \n",
            "Epoch: [3][80/366] Elapsed 2m 39s (remain 9m 19s) Loss: 0.0918(0.1004) Grad: 160428.2969  LR: 0.00001176  \n",
            "Epoch: [3][100/366] Elapsed 3m 17s (remain 8m 39s) Loss: 0.0877(0.1000) Grad: 124458.2734  LR: 0.00001143  \n",
            "Epoch: [3][120/366] Elapsed 3m 53s (remain 7m 52s) Loss: 0.1279(0.0996) Grad: 80006.8047  LR: 0.00001109  \n",
            "Epoch: [3][140/366] Elapsed 4m 30s (remain 7m 11s) Loss: 0.1596(0.1010) Grad: 140786.4219  LR: 0.00001074  \n",
            "Epoch: [3][160/366] Elapsed 5m 6s (remain 6m 30s) Loss: 0.1403(0.1001) Grad: 106502.0156  LR: 0.00001040  \n",
            "Epoch: [3][180/366] Elapsed 5m 38s (remain 5m 46s) Loss: 0.0829(0.0996) Grad: 131603.5156  LR: 0.00001006  \n",
            "Epoch: [3][200/366] Elapsed 6m 13s (remain 5m 6s) Loss: 0.0854(0.1001) Grad: 143363.2188  LR: 0.00000972  \n",
            "Epoch: [3][220/366] Elapsed 6m 54s (remain 4m 32s) Loss: 0.1149(0.1006) Grad: 148829.2500  LR: 0.00000937  \n",
            "Epoch: [3][240/366] Elapsed 7m 30s (remain 3m 53s) Loss: 0.1875(0.1006) Grad: 259685.5625  LR: 0.00000903  \n",
            "Epoch: [3][260/366] Elapsed 8m 7s (remain 3m 16s) Loss: 0.1097(0.1001) Grad: 125524.6328  LR: 0.00000869  \n",
            "Epoch: [3][280/366] Elapsed 8m 48s (remain 2m 39s) Loss: 0.0832(0.0995) Grad: 112560.5859  LR: 0.00000835  \n",
            "Epoch: [3][300/366] Elapsed 9m 26s (remain 2m 2s) Loss: 0.1463(0.0998) Grad: 83995.1484  LR: 0.00000802  \n",
            "Epoch: [3][320/366] Elapsed 9m 59s (remain 1m 24s) Loss: 0.0686(0.0995) Grad: 78144.0469  LR: 0.00000768  \n",
            "Epoch: [3][340/366] Elapsed 10m 35s (remain 0m 46s) Loss: 0.0915(0.0993) Grad: 112543.5156  LR: 0.00000735  \n",
            "Epoch: [3][360/366] Elapsed 11m 6s (remain 0m 9s) Loss: 0.1101(0.0994) Grad: 160912.7031  LR: 0.00000702  \n",
            "Epoch: [3][365/366] Elapsed 11m 14s (remain 0m 0s) Loss: 0.0601(0.0991) Grad: 54805.0703  LR: 0.00000694  \n",
            "EVAL: [0/62] Elapsed 0m 0s (remain 0m 56s) Loss: 0.1322(0.1322) \n",
            "EVAL: [20/62] Elapsed 0m 21s (remain 0m 42s) Loss: 0.0995(0.1109) \n",
            "EVAL: [40/62] Elapsed 0m 44s (remain 0m 22s) Loss: 0.1008(0.1039) \n",
            "EVAL: [60/62] Elapsed 1m 8s (remain 0m 1s) Loss: 0.0903(0.1048) \n",
            "EVAL: [61/62] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1012(0.1048) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0991  avg_val_loss: 0.1048  time: 743s\n",
            "Epoch 3 - Score: 0.4585  Scores: [0.49362733493896865, 0.45784587692501705, 0.4183949132215376, 0.44939004709746744, 0.48841406337862364, 0.443480341860978]\n",
            "Epoch 3 - Save Best Score: 0.4585 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [4][0/366] Elapsed 0m 2s (remain 13m 29s) Loss: 0.0996(0.0996) Grad: inf  LR: 0.00000692  \n",
            "Epoch: [4][20/366] Elapsed 0m 42s (remain 11m 37s) Loss: 0.0757(0.0882) Grad: 103428.2812  LR: 0.00000660  \n",
            "Epoch: [4][40/366] Elapsed 1m 19s (remain 10m 26s) Loss: 0.1130(0.0914) Grad: 100600.4453  LR: 0.00000628  \n",
            "Epoch: [4][60/366] Elapsed 1m 48s (remain 9m 4s) Loss: 0.0695(0.0940) Grad: 36366.2344  LR: 0.00000596  \n",
            "Epoch: [4][80/366] Elapsed 2m 26s (remain 8m 35s) Loss: 0.0945(0.0931) Grad: 98778.5703  LR: 0.00000565  \n",
            "Epoch: [4][100/366] Elapsed 2m 59s (remain 7m 50s) Loss: 0.0901(0.0936) Grad: 77871.5391  LR: 0.00000535  \n",
            "Epoch: [4][120/366] Elapsed 3m 37s (remain 7m 19s) Loss: 0.0675(0.0935) Grad: 136270.5469  LR: 0.00000504  \n",
            "Epoch: [4][140/366] Elapsed 4m 8s (remain 6m 35s) Loss: 0.0617(0.0931) Grad: 95850.9219  LR: 0.00000475  \n",
            "Epoch: [4][160/366] Elapsed 4m 49s (remain 6m 8s) Loss: 0.1134(0.0926) Grad: 150634.1250  LR: 0.00000446  \n",
            "Epoch: [4][180/366] Elapsed 5m 27s (remain 5m 34s) Loss: 0.1007(0.0930) Grad: 56798.1172  LR: 0.00000418  \n",
            "Epoch: [4][200/366] Elapsed 6m 9s (remain 5m 2s) Loss: 0.0767(0.0926) Grad: 105236.2188  LR: 0.00000390  \n",
            "Epoch: [4][220/366] Elapsed 6m 44s (remain 4m 25s) Loss: 0.0849(0.0919) Grad: 139858.4688  LR: 0.00000364  \n",
            "Epoch: [4][240/366] Elapsed 7m 24s (remain 3m 50s) Loss: 0.1310(0.0921) Grad: 152690.7969  LR: 0.00000338  \n",
            "Epoch: [4][260/366] Elapsed 8m 6s (remain 3m 15s) Loss: 0.0694(0.0930) Grad: 112112.8125  LR: 0.00000312  \n",
            "Epoch: [4][280/366] Elapsed 8m 43s (remain 2m 38s) Loss: 0.0607(0.0930) Grad: 81392.4844  LR: 0.00000288  \n",
            "Epoch: [4][300/366] Elapsed 9m 17s (remain 2m 0s) Loss: 0.0737(0.0928) Grad: 70665.2812  LR: 0.00000264  \n",
            "Epoch: [4][320/366] Elapsed 9m 50s (remain 1m 22s) Loss: 0.0648(0.0925) Grad: 58024.7148  LR: 0.00000241  \n",
            "Epoch: [4][340/366] Elapsed 10m 29s (remain 0m 46s) Loss: 0.0782(0.0926) Grad: 95401.8984  LR: 0.00000219  \n",
            "Epoch: [4][360/366] Elapsed 11m 2s (remain 0m 9s) Loss: 0.0675(0.0924) Grad: 68467.4297  LR: 0.00000199  \n",
            "Epoch: [4][365/366] Elapsed 11m 9s (remain 0m 0s) Loss: 0.0919(0.0926) Grad: 72216.5000  LR: 0.00000193  \n",
            "EVAL: [0/62] Elapsed 0m 0s (remain 0m 50s) Loss: 0.1320(0.1320) \n",
            "EVAL: [20/62] Elapsed 0m 21s (remain 0m 42s) Loss: 0.0948(0.1103) \n",
            "EVAL: [40/62] Elapsed 0m 44s (remain 0m 22s) Loss: 0.1009(0.1030) \n",
            "EVAL: [60/62] Elapsed 1m 8s (remain 0m 1s) Loss: 0.0861(0.1036) \n",
            "EVAL: [61/62] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1028(0.1036) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0926  avg_val_loss: 0.1036  time: 739s\n",
            "Epoch 4 - Score: 0.4559  Scores: [0.48864049488221334, 0.45854790837407083, 0.41772157913792524, 0.4486687917330637, 0.48005485601132636, 0.4416262970791042]\n",
            "Epoch 4 - Save Best Score: 0.4559 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [5][0/366] Elapsed 0m 1s (remain 10m 13s) Loss: 0.0739(0.0739) Grad: 182782.2344  LR: 0.00000192  \n",
            "Epoch: [5][20/366] Elapsed 0m 34s (remain 9m 27s) Loss: 0.0907(0.0865) Grad: 95687.1953  LR: 0.00000173  \n",
            "Epoch: [5][40/366] Elapsed 1m 8s (remain 9m 6s) Loss: 0.0721(0.0861) Grad: 60288.7539  LR: 0.00000154  \n",
            "Epoch: [5][60/366] Elapsed 1m 44s (remain 8m 42s) Loss: 0.0430(0.0877) Grad: 80165.3203  LR: 0.00000136  \n",
            "Epoch: [5][80/366] Elapsed 2m 21s (remain 8m 18s) Loss: 0.1048(0.0895) Grad: 117599.4297  LR: 0.00000119  \n",
            "Epoch: [5][100/366] Elapsed 2m 58s (remain 7m 49s) Loss: 0.0725(0.0891) Grad: 55358.4805  LR: 0.00000104  \n",
            "Epoch: [5][120/366] Elapsed 3m 31s (remain 7m 8s) Loss: 0.1315(0.0875) Grad: 80968.9766  LR: 0.00000089  \n",
            "Epoch: [5][140/366] Elapsed 4m 11s (remain 6m 41s) Loss: 0.0650(0.0874) Grad: 122198.6719  LR: 0.00000075  \n",
            "Epoch: [5][160/366] Elapsed 4m 54s (remain 6m 14s) Loss: 0.1103(0.0878) Grad: 195406.8125  LR: 0.00000063  \n",
            "Epoch: [5][180/366] Elapsed 5m 31s (remain 5m 38s) Loss: 0.0645(0.0877) Grad: 118723.3281  LR: 0.00000051  \n",
            "Epoch: [5][200/366] Elapsed 6m 10s (remain 5m 4s) Loss: 0.1072(0.0876) Grad: 74140.8750  LR: 0.00000041  \n",
            "Epoch: [5][220/366] Elapsed 6m 48s (remain 4m 27s) Loss: 0.0867(0.0881) Grad: 101644.8984  LR: 0.00000032  \n",
            "Epoch: [5][240/366] Elapsed 7m 31s (remain 3m 53s) Loss: 0.0806(0.0886) Grad: 195345.7344  LR: 0.00000024  \n",
            "Epoch: [5][260/366] Elapsed 8m 2s (remain 3m 14s) Loss: 0.0756(0.0888) Grad: 86867.7734  LR: 0.00000017  \n",
            "Epoch: [5][280/366] Elapsed 8m 40s (remain 2m 37s) Loss: 0.0951(0.0885) Grad: 53005.6758  LR: 0.00000011  \n",
            "Epoch: [5][300/366] Elapsed 9m 20s (remain 2m 0s) Loss: 0.0740(0.0886) Grad: 87305.0234  LR: 0.00000007  \n",
            "Epoch: [5][320/366] Elapsed 9m 56s (remain 1m 23s) Loss: 0.0923(0.0886) Grad: 133510.8906  LR: 0.00000003  \n",
            "Epoch: [5][340/366] Elapsed 10m 31s (remain 0m 46s) Loss: 0.0471(0.0883) Grad: 103025.0859  LR: 0.00000001  \n",
            "Epoch: [5][360/366] Elapsed 11m 2s (remain 0m 9s) Loss: 0.0758(0.0883) Grad: 58162.2148  LR: 0.00000000  \n",
            "Epoch: [5][365/366] Elapsed 11m 9s (remain 0m 0s) Loss: 0.0768(0.0883) Grad: 178626.5938  LR: 0.00000000  \n",
            "EVAL: [0/62] Elapsed 0m 0s (remain 0m 50s) Loss: 0.1300(0.1300) \n",
            "EVAL: [20/62] Elapsed 0m 21s (remain 0m 42s) Loss: 0.0962(0.1103) \n",
            "EVAL: [40/62] Elapsed 0m 44s (remain 0m 22s) Loss: 0.0996(0.1030) \n",
            "EVAL: [60/62] Elapsed 1m 8s (remain 0m 1s) Loss: 0.0873(0.1038) \n",
            "EVAL: [61/62] Elapsed 1m 8s (remain 0m 0s) Loss: 0.0971(0.1038) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0883  avg_val_loss: 0.1038  time: 739s\n",
            "Epoch 5 - Score: 0.4565  Scores: [0.48873735754230974, 0.4598431652748847, 0.41762654273580974, 0.4497675341817808, 0.4805897329796801, 0.4424896867091492]\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.4559  Scores: [0.48864049488221334, 0.45854790837407083, 0.41772157913792524, 0.4486687917330637, 0.48005485601132636, 0.4416262970791042]\n",
            "========== fold: 3 training ==========\n",
            "DebertaV2Config {\n",
            "  \"_name_or_path\": \"/kaggle/input/debertav3base/\",\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at /kaggle/input/debertav3base/ were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/366] Elapsed 0m 1s (remain 10m 58s) Loss: 2.4907(2.4907) Grad: inf  LR: 0.00002000  \n",
            "Epoch: [1][20/366] Elapsed 0m 46s (remain 12m 42s) Loss: 0.3629(1.3937) Grad: 164468.1875  LR: 0.00001999  \n",
            "Epoch: [1][40/366] Elapsed 1m 23s (remain 10m 59s) Loss: 0.1169(0.8070) Grad: 48368.6953  LR: 0.00001998  \n",
            "Epoch: [1][60/366] Elapsed 1m 52s (remain 9m 24s) Loss: 0.1376(0.5868) Grad: 73096.2500  LR: 0.00001995  \n",
            "Epoch: [1][80/366] Elapsed 2m 26s (remain 8m 36s) Loss: 0.0934(0.4781) Grad: 55550.5781  LR: 0.00001990  \n",
            "Epoch: [1][100/366] Elapsed 3m 7s (remain 8m 11s) Loss: 0.0784(0.4106) Grad: 35145.7031  LR: 0.00001985  \n",
            "Epoch: [1][120/366] Elapsed 3m 45s (remain 7m 36s) Loss: 0.0938(0.3637) Grad: 40209.0469  LR: 0.00001979  \n",
            "Epoch: [1][140/366] Elapsed 4m 22s (remain 6m 58s) Loss: 0.1643(0.3314) Grad: 74800.8906  LR: 0.00001971  \n",
            "Epoch: [1][160/366] Elapsed 4m 56s (remain 6m 17s) Loss: 0.1731(0.3046) Grad: 80199.1094  LR: 0.00001962  \n",
            "Epoch: [1][180/366] Elapsed 5m 33s (remain 5m 40s) Loss: 0.2139(0.2852) Grad: 49906.2070  LR: 0.00001952  \n",
            "Epoch: [1][200/366] Elapsed 6m 10s (remain 5m 4s) Loss: 0.0909(0.2692) Grad: 38638.4297  LR: 0.00001941  \n",
            "Epoch: [1][220/366] Elapsed 6m 37s (remain 4m 20s) Loss: 0.1392(0.2555) Grad: 99233.5703  LR: 0.00001929  \n",
            "Epoch: [1][240/366] Elapsed 7m 11s (remain 3m 43s) Loss: 0.1536(0.2449) Grad: 88540.4688  LR: 0.00001916  \n",
            "Epoch: [1][260/366] Elapsed 7m 45s (remain 3m 7s) Loss: 0.1112(0.2369) Grad: 87282.0703  LR: 0.00001902  \n",
            "Epoch: [1][280/366] Elapsed 8m 20s (remain 2m 31s) Loss: 0.1467(0.2289) Grad: 89079.8359  LR: 0.00001886  \n",
            "Epoch: [1][300/366] Elapsed 8m 59s (remain 1m 56s) Loss: 0.1252(0.2222) Grad: 81114.0938  LR: 0.00001870  \n",
            "Epoch: [1][320/366] Elapsed 9m 37s (remain 1m 20s) Loss: 0.0988(0.2155) Grad: 66694.6016  LR: 0.00001852  \n",
            "Epoch: [1][340/366] Elapsed 10m 11s (remain 0m 44s) Loss: 0.1359(0.2102) Grad: 49871.5742  LR: 0.00001834  \n",
            "Epoch: [1][360/366] Elapsed 10m 43s (remain 0m 8s) Loss: 0.1425(0.2056) Grad: 59414.2227  LR: 0.00001815  \n",
            "Epoch: [1][365/366] Elapsed 10m 50s (remain 0m 0s) Loss: 0.1564(0.2043) Grad: 38180.5000  LR: 0.00001810  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 22s) Loss: 0.0838(0.0838) \n",
            "EVAL: [20/62] Elapsed 0m 25s (remain 0m 49s) Loss: 0.0812(0.1069) \n",
            "EVAL: [40/62] Elapsed 0m 51s (remain 0m 26s) Loss: 0.1176(0.1108) \n",
            "EVAL: [60/62] Elapsed 1m 19s (remain 0m 1s) Loss: 0.0911(0.1112) \n",
            "EVAL: [61/62] Elapsed 1m 19s (remain 0m 0s) Loss: 0.0479(0.1111) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2043  avg_val_loss: 0.1111  time: 729s\n",
            "Epoch 1 - Score: 0.4722  Scores: [0.5080878678481079, 0.487717426449642, 0.4342749540602561, 0.45921968108685807, 0.4849055988885666, 0.4588816029517034]\n",
            "Epoch 1 - Save Best Score: 0.4722 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/366] Elapsed 0m 1s (remain 10m 5s) Loss: 0.1186(0.1186) Grad: 156966.3438  LR: 0.00001809  \n",
            "Epoch: [2][20/366] Elapsed 0m 43s (remain 11m 48s) Loss: 0.0712(0.1022) Grad: 111189.9141  LR: 0.00001788  \n",
            "Epoch: [2][40/366] Elapsed 1m 20s (remain 10m 37s) Loss: 0.0707(0.1048) Grad: 116930.9844  LR: 0.00001766  \n",
            "Epoch: [2][60/366] Elapsed 2m 0s (remain 10m 1s) Loss: 0.0966(0.1048) Grad: 102143.5312  LR: 0.00001744  \n",
            "Epoch: [2][80/366] Elapsed 2m 36s (remain 9m 10s) Loss: 0.0913(0.1054) Grad: 71865.5625  LR: 0.00001721  \n",
            "Epoch: [2][100/366] Elapsed 3m 9s (remain 8m 16s) Loss: 0.0937(0.1045) Grad: 110377.0000  LR: 0.00001696  \n",
            "Epoch: [2][120/366] Elapsed 3m 48s (remain 7m 42s) Loss: 0.0894(0.1056) Grad: 149451.3906  LR: 0.00001671  \n",
            "Epoch: [2][140/366] Elapsed 4m 21s (remain 6m 57s) Loss: 0.1088(0.1039) Grad: 67825.3828  LR: 0.00001646  \n",
            "Epoch: [2][160/366] Elapsed 4m 56s (remain 6m 17s) Loss: 0.0982(0.1036) Grad: 110690.3672  LR: 0.00001619  \n",
            "Epoch: [2][180/366] Elapsed 5m 34s (remain 5m 42s) Loss: 0.0614(0.1028) Grad: 82951.6953  LR: 0.00001592  \n",
            "Epoch: [2][200/366] Elapsed 6m 8s (remain 5m 2s) Loss: 0.0979(0.1027) Grad: 130164.3438  LR: 0.00001564  \n",
            "Epoch: [2][220/366] Elapsed 6m 41s (remain 4m 23s) Loss: 0.0834(0.1029) Grad: 181911.7969  LR: 0.00001535  \n",
            "Epoch: [2][240/366] Elapsed 7m 15s (remain 3m 45s) Loss: 0.1185(0.1028) Grad: 82645.2188  LR: 0.00001506  \n",
            "Epoch: [2][260/366] Elapsed 7m 49s (remain 3m 8s) Loss: 0.1493(0.1026) Grad: 176836.3125  LR: 0.00001476  \n",
            "Epoch: [2][280/366] Elapsed 8m 24s (remain 2m 32s) Loss: 0.1092(0.1030) Grad: 76289.6406  LR: 0.00001446  \n",
            "Epoch: [2][300/366] Elapsed 8m 57s (remain 1m 56s) Loss: 0.1230(0.1040) Grad: 86049.4375  LR: 0.00001415  \n",
            "Epoch: [2][320/366] Elapsed 9m 31s (remain 1m 20s) Loss: 0.1045(0.1042) Grad: 154313.5156  LR: 0.00001383  \n",
            "Epoch: [2][340/366] Elapsed 10m 4s (remain 0m 44s) Loss: 0.1350(0.1041) Grad: 67081.6641  LR: 0.00001351  \n",
            "Epoch: [2][360/366] Elapsed 10m 38s (remain 0m 8s) Loss: 0.0883(0.1041) Grad: 82124.1953  LR: 0.00001319  \n",
            "Epoch: [2][365/366] Elapsed 10m 45s (remain 0m 0s) Loss: 0.0821(0.1039) Grad: 72130.1875  LR: 0.00001311  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 7s) Loss: 0.1004(0.1004) \n",
            "EVAL: [20/62] Elapsed 0m 24s (remain 0m 48s) Loss: 0.0975(0.1068) \n",
            "EVAL: [40/62] Elapsed 0m 50s (remain 0m 25s) Loss: 0.0841(0.1079) \n",
            "EVAL: [60/62] Elapsed 1m 17s (remain 0m 1s) Loss: 0.0874(0.1085) \n",
            "EVAL: [61/62] Elapsed 1m 18s (remain 0m 0s) Loss: 0.0341(0.1083) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1039  avg_val_loss: 0.1083  time: 724s\n",
            "Epoch 2 - Score: 0.4664  Scores: [0.5107878228886483, 0.45890497554896037, 0.44286678202107393, 0.4613802784780657, 0.477589243552756, 0.44684256122961546]\n",
            "Epoch 2 - Save Best Score: 0.4664 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [3][0/366] Elapsed 0m 3s (remain 22m 59s) Loss: 0.0951(0.0951) Grad: 133471.4375  LR: 0.00001309  \n",
            "Epoch: [3][20/366] Elapsed 0m 39s (remain 10m 42s) Loss: 0.0909(0.0913) Grad: 218474.1094  LR: 0.00001277  \n",
            "Epoch: [3][40/366] Elapsed 1m 14s (remain 9m 48s) Loss: 0.1155(0.0916) Grad: 136418.1250  LR: 0.00001243  \n",
            "Epoch: [3][60/366] Elapsed 1m 51s (remain 9m 16s) Loss: 0.1604(0.0918) Grad: 182349.3594  LR: 0.00001210  \n",
            "Epoch: [3][80/366] Elapsed 2m 28s (remain 8m 43s) Loss: 0.0563(0.0921) Grad: 87213.9375  LR: 0.00001176  \n",
            "Epoch: [3][100/366] Elapsed 3m 5s (remain 8m 6s) Loss: 0.2013(0.0928) Grad: 157039.6719  LR: 0.00001143  \n",
            "Epoch: [3][120/366] Elapsed 3m 45s (remain 7m 36s) Loss: 0.0891(0.0930) Grad: 109520.4922  LR: 0.00001109  \n",
            "Epoch: [3][140/366] Elapsed 4m 17s (remain 6m 51s) Loss: 0.1218(0.0925) Grad: 102945.6562  LR: 0.00001074  \n",
            "Epoch: [3][160/366] Elapsed 4m 53s (remain 6m 13s) Loss: 0.0910(0.0952) Grad: 115894.2969  LR: 0.00001040  \n",
            "Epoch: [3][180/366] Elapsed 5m 32s (remain 5m 39s) Loss: 0.0607(0.0957) Grad: 66829.0859  LR: 0.00001006  \n",
            "Epoch: [3][200/366] Elapsed 6m 12s (remain 5m 5s) Loss: 0.0752(0.0954) Grad: 85973.0469  LR: 0.00000972  \n",
            "Epoch: [3][220/366] Elapsed 6m 46s (remain 4m 27s) Loss: 0.1047(0.0946) Grad: 72544.1797  LR: 0.00000937  \n",
            "Epoch: [3][240/366] Elapsed 7m 21s (remain 3m 49s) Loss: 0.0560(0.0941) Grad: 42139.7812  LR: 0.00000903  \n",
            "Epoch: [3][260/366] Elapsed 7m 52s (remain 3m 10s) Loss: 0.1233(0.0942) Grad: 190700.1406  LR: 0.00000869  \n",
            "Epoch: [3][280/366] Elapsed 8m 25s (remain 2m 32s) Loss: 0.1320(0.0945) Grad: 202847.8906  LR: 0.00000835  \n",
            "Epoch: [3][300/366] Elapsed 8m 59s (remain 1m 56s) Loss: 0.1089(0.0943) Grad: 96414.9844  LR: 0.00000802  \n",
            "Epoch: [3][320/366] Elapsed 9m 31s (remain 1m 20s) Loss: 0.1031(0.0942) Grad: 103738.8594  LR: 0.00000768  \n",
            "Epoch: [3][340/366] Elapsed 10m 9s (remain 0m 44s) Loss: 0.0684(0.0944) Grad: 78164.0391  LR: 0.00000735  \n",
            "Epoch: [3][360/366] Elapsed 10m 43s (remain 0m 8s) Loss: 0.1310(0.0942) Grad: 118904.5312  LR: 0.00000702  \n",
            "Epoch: [3][365/366] Elapsed 10m 50s (remain 0m 0s) Loss: 0.0639(0.0941) Grad: 68867.7344  LR: 0.00000694  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 7s) Loss: 0.0817(0.0817) \n",
            "EVAL: [20/62] Elapsed 0m 24s (remain 0m 48s) Loss: 0.0835(0.0995) \n",
            "EVAL: [40/62] Elapsed 0m 50s (remain 0m 26s) Loss: 0.0894(0.1030) \n",
            "EVAL: [60/62] Elapsed 1m 18s (remain 0m 1s) Loss: 0.0791(0.1036) \n",
            "EVAL: [61/62] Elapsed 1m 18s (remain 0m 0s) Loss: 0.0398(0.1035) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0941  avg_val_loss: 0.1035  time: 729s\n",
            "Epoch 3 - Score: 0.4557  Scores: [0.49409977667841654, 0.4513598118150845, 0.4201327976858789, 0.45634952975229137, 0.46602727507509195, 0.4460431630969256]\n",
            "Epoch 3 - Save Best Score: 0.4557 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [4][0/366] Elapsed 0m 1s (remain 11m 59s) Loss: 0.0747(0.0747) Grad: 172744.1875  LR: 0.00000692  \n",
            "Epoch: [4][20/366] Elapsed 0m 42s (remain 11m 32s) Loss: 0.0965(0.0808) Grad: 216642.2344  LR: 0.00000660  \n",
            "Epoch: [4][40/366] Elapsed 1m 17s (remain 10m 10s) Loss: 0.0774(0.0802) Grad: 168718.2188  LR: 0.00000628  \n",
            "Epoch: [4][60/366] Elapsed 1m 50s (remain 9m 12s) Loss: 0.0753(0.0826) Grad: 191281.5000  LR: 0.00000596  \n",
            "Epoch: [4][80/366] Elapsed 2m 23s (remain 8m 23s) Loss: 0.0643(0.0855) Grad: 134483.3438  LR: 0.00000565  \n",
            "Epoch: [4][100/366] Elapsed 2m 59s (remain 7m 50s) Loss: 0.0815(0.0876) Grad: 126995.8672  LR: 0.00000535  \n",
            "Epoch: [4][120/366] Elapsed 3m 36s (remain 7m 17s) Loss: 0.0827(0.0883) Grad: 171864.2031  LR: 0.00000504  \n",
            "Epoch: [4][140/366] Elapsed 4m 10s (remain 6m 39s) Loss: 0.1289(0.0878) Grad: 301860.0000  LR: 0.00000475  \n",
            "Epoch: [4][160/366] Elapsed 4m 44s (remain 6m 1s) Loss: 0.1077(0.0875) Grad: 207718.3125  LR: 0.00000446  \n",
            "Epoch: [4][180/366] Elapsed 5m 22s (remain 5m 29s) Loss: 0.0758(0.0865) Grad: 171776.2188  LR: 0.00000418  \n",
            "Epoch: [4][200/366] Elapsed 5m 54s (remain 4m 51s) Loss: 0.1022(0.0859) Grad: 176846.2969  LR: 0.00000390  \n",
            "Epoch: [4][220/366] Elapsed 6m 32s (remain 4m 17s) Loss: 0.0728(0.0858) Grad: 123448.9609  LR: 0.00000364  \n",
            "Epoch: [4][240/366] Elapsed 7m 4s (remain 3m 39s) Loss: 0.0919(0.0860) Grad: 112940.9531  LR: 0.00000338  \n",
            "Epoch: [4][260/366] Elapsed 7m 37s (remain 3m 4s) Loss: 0.0931(0.0856) Grad: 80614.7266  LR: 0.00000312  \n",
            "Epoch: [4][280/366] Elapsed 8m 11s (remain 2m 28s) Loss: 0.0955(0.0864) Grad: 119678.0234  LR: 0.00000288  \n",
            "Epoch: [4][300/366] Elapsed 8m 47s (remain 1m 53s) Loss: 0.1185(0.0864) Grad: 108264.0781  LR: 0.00000264  \n",
            "Epoch: [4][320/366] Elapsed 9m 24s (remain 1m 19s) Loss: 0.1200(0.0862) Grad: 91527.3906  LR: 0.00000241  \n",
            "Epoch: [4][340/366] Elapsed 9m 58s (remain 0m 43s) Loss: 0.1109(0.0859) Grad: 157551.0156  LR: 0.00000219  \n",
            "Epoch: [4][360/366] Elapsed 10m 33s (remain 0m 8s) Loss: 0.0520(0.0854) Grad: 117848.0781  LR: 0.00000199  \n",
            "Epoch: [4][365/366] Elapsed 10m 41s (remain 0m 0s) Loss: 0.0828(0.0855) Grad: 77821.4609  LR: 0.00000193  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 7s) Loss: 0.0837(0.0837) \n",
            "EVAL: [20/62] Elapsed 0m 24s (remain 0m 48s) Loss: 0.0764(0.0989) \n",
            "EVAL: [40/62] Elapsed 0m 50s (remain 0m 25s) Loss: 0.0905(0.1025) \n",
            "EVAL: [60/62] Elapsed 1m 18s (remain 0m 1s) Loss: 0.0847(0.1034) \n",
            "EVAL: [61/62] Elapsed 1m 18s (remain 0m 0s) Loss: 0.0426(0.1033) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0855  avg_val_loss: 0.1033  time: 720s\n",
            "Epoch 4 - Score: 0.4554  Scores: [0.49056861926216083, 0.4522281905279417, 0.4199962631727616, 0.45556276268884976, 0.4675090988182876, 0.44633788321852924]\n",
            "Epoch 4 - Save Best Score: 0.4554 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [5][0/366] Elapsed 0m 1s (remain 10m 19s) Loss: 0.0627(0.0627) Grad: 220466.4844  LR: 0.00000192  \n",
            "Epoch: [5][20/366] Elapsed 0m 46s (remain 12m 50s) Loss: 0.0950(0.0889) Grad: 119457.3281  LR: 0.00000173  \n",
            "Epoch: [5][40/366] Elapsed 1m 20s (remain 10m 34s) Loss: 0.0815(0.0863) Grad: 161190.3438  LR: 0.00000154  \n",
            "Epoch: [5][60/366] Elapsed 1m 48s (remain 9m 1s) Loss: 0.0677(0.0840) Grad: 128656.9609  LR: 0.00000136  \n",
            "Epoch: [5][80/366] Elapsed 2m 24s (remain 8m 29s) Loss: 0.0742(0.0850) Grad: 154329.0469  LR: 0.00000119  \n",
            "Epoch: [5][100/366] Elapsed 2m 57s (remain 7m 44s) Loss: 0.0556(0.0840) Grad: 130542.9688  LR: 0.00000104  \n",
            "Epoch: [5][120/366] Elapsed 3m 28s (remain 7m 1s) Loss: 0.0762(0.0836) Grad: 186969.7031  LR: 0.00000089  \n",
            "Epoch: [5][140/366] Elapsed 4m 7s (remain 6m 34s) Loss: 0.1138(0.0834) Grad: 177147.8125  LR: 0.00000075  \n",
            "Epoch: [5][160/366] Elapsed 4m 45s (remain 6m 3s) Loss: 0.0664(0.0826) Grad: 116900.3750  LR: 0.00000063  \n",
            "Epoch: [5][180/366] Elapsed 5m 19s (remain 5m 26s) Loss: 0.0503(0.0821) Grad: 81916.2656  LR: 0.00000051  \n",
            "Epoch: [5][200/366] Elapsed 5m 53s (remain 4m 49s) Loss: 0.0469(0.0819) Grad: 167203.9688  LR: 0.00000041  \n",
            "Epoch: [5][220/366] Elapsed 6m 30s (remain 4m 16s) Loss: 0.1532(0.0822) Grad: 297967.6875  LR: 0.00000032  \n",
            "Epoch: [5][240/366] Elapsed 7m 3s (remain 3m 39s) Loss: 0.0671(0.0819) Grad: 130321.1641  LR: 0.00000024  \n",
            "Epoch: [5][260/366] Elapsed 7m 42s (remain 3m 6s) Loss: 0.0791(0.0817) Grad: 187625.5000  LR: 0.00000017  \n",
            "Epoch: [5][280/366] Elapsed 8m 20s (remain 2m 31s) Loss: 0.0541(0.0816) Grad: 105076.0078  LR: 0.00000011  \n",
            "Epoch: [5][300/366] Elapsed 8m 53s (remain 1m 55s) Loss: 0.0752(0.0811) Grad: 136802.4375  LR: 0.00000007  \n",
            "Epoch: [5][320/366] Elapsed 9m 26s (remain 1m 19s) Loss: 0.0565(0.0811) Grad: 117030.6797  LR: 0.00000003  \n",
            "Epoch: [5][340/366] Elapsed 10m 13s (remain 0m 44s) Loss: 0.0872(0.0810) Grad: 288060.2188  LR: 0.00000001  \n",
            "Epoch: [5][360/366] Elapsed 10m 45s (remain 0m 8s) Loss: 0.0916(0.0814) Grad: 144262.0625  LR: 0.00000000  \n",
            "Epoch: [5][365/366] Elapsed 10m 52s (remain 0m 0s) Loss: 0.0618(0.0814) Grad: 125423.3906  LR: 0.00000000  \n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 8s) Loss: 0.0838(0.0838) \n",
            "EVAL: [20/62] Elapsed 0m 24s (remain 0m 48s) Loss: 0.0763(0.0988) \n",
            "EVAL: [40/62] Elapsed 0m 50s (remain 0m 25s) Loss: 0.0889(0.1018) \n",
            "EVAL: [60/62] Elapsed 1m 18s (remain 0m 1s) Loss: 0.0846(0.1028) \n",
            "EVAL: [61/62] Elapsed 1m 18s (remain 0m 0s) Loss: 0.0393(0.1026) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0814  avg_val_loss: 0.1026  time: 731s\n",
            "Epoch 5 - Score: 0.4538  Scores: [0.4901420855573412, 0.4502627898082984, 0.41639586145381474, 0.45396874903176276, 0.46711645356694903, 0.44471446197691805]\n",
            "Epoch 5 - Save Best Score: 0.4538 Model\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.4538  Scores: [0.4901420855573412, 0.4502627898082984, 0.41639586145381474, 0.45396874903176276, 0.46711645356694903, 0.44471446197691805]\n",
            "========== CV ==========\n",
            "Score: 0.4569  Scores: [0.4908865030819827, 0.45083427691211003, 0.41748033615805047, 0.456507211159442, 0.4774611486443539, 0.44828430864670243]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    if CFG.train:\n",
        "        oof_df = pd.DataFrame()\n",
        "        for fold in range(CFG.n_fold):\n",
        "            if fold in CFG.trn_fold:\n",
        "                _oof_df = train_loop(train, fold)\n",
        "                oof_df = pd.concat([oof_df, _oof_df])\n",
        "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "                get_result(_oof_df)\n",
        "        oof_df = oof_df.reset_index(drop=True)\n",
        "        LOGGER.info(f\"========== CV ==========\")\n",
        "        get_result(oof_df)\n",
        "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "========== fold: 0 result ==========\n",
        "\n",
        "Score: 0.4535 \n",
        "\n",
        "========== fold: 1 result ==========\n",
        "\n",
        "Score: 0.4643  \n",
        "\n",
        "========== fold: 2 result ==========\n",
        "\n",
        "Score: 0.4559  \n",
        "\n",
        "========== fold: 3 result ==========\n",
        "\n",
        "Score: 0.4538  \n",
        "\n",
        "\n",
        "**After averaging the scores:**\n",
        "\n",
        "**========== CV ==========**\n",
        "\n",
        "**Score: 0.4569**\n",
        "\n",
        "Each particular error score: \n",
        "\n",
        "[0.4908865030819827, 0.45083427691211003, 0.41748033615805047, 0.456507211159442, 0.4774611486443539, 0.44828430864670243] \n",
        "\n",
        "for cohesion, syntax, vocabulary, phraseology, grammar, conventions"
      ],
      "metadata": {
        "id": "Ay2e0Qv7G9qE"
      },
      "id": "Ay2e0Qv7G9qE"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9u3fT-FpQtC9"
      },
      "id": "9u3fT-FpQtC9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model inference"
      ],
      "metadata": {
        "id": "DsHeat1LDO8H"
      },
      "id": "DsHeat1LDO8H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd51633a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T13:29:30.960784Z",
          "iopub.status.busy": "2022-11-24T13:29:30.959365Z",
          "iopub.status.idle": "2022-11-24T13:29:30.965846Z",
          "shell.execute_reply": "2022-11-24T13:29:30.964978Z"
        },
        "papermill": {
          "duration": 0.046887,
          "end_time": "2022-11-24T13:29:30.967746",
          "exception": false,
          "start_time": "2022-11-24T13:29:30.920859",
          "status": "completed"
        },
        "tags": [],
        "id": "bd51633a"
      },
      "outputs": [],
      "source": [
        "# CFG class for inference\n",
        "\n",
        "class CFG:\n",
        "    num_workers=4\n",
        "    path=\"./\"\n",
        "    config_path=path+'config.pth'\n",
        "    model=\"/kaggle/input/debertav3base/\"\n",
        "    gradient_checkpointing=False\n",
        "    batch_size=24\n",
        "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "    seed=42\n",
        "    n_fold=4\n",
        "    trn_fold=[0, 1, 2, 3]\n",
        "\n",
        "CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading test sample"
      ],
      "metadata": {
        "id": "1YTVB-5SDUa5"
      },
      "id": "1YTVB-5SDUa5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4165d7dd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T13:29:31.505524Z",
          "iopub.status.busy": "2022-11-24T13:29:31.504128Z",
          "iopub.status.idle": "2022-11-24T13:29:31.542265Z",
          "shell.execute_reply": "2022-11-24T13:29:31.541257Z"
        },
        "papermill": {
          "duration": 0.0805,
          "end_time": "2022-11-24T13:29:31.544370",
          "exception": false,
          "start_time": "2022-11-24T13:29:31.463870",
          "status": "completed"
        },
        "tags": [],
        "id": "4165d7dd",
        "outputId": "cb0960a5-35cf-408a-c6b0-9c0f2dc49126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test.shape: (3, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>when a person has no experience on a job their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>Do you think students would benefit from being...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>Thomas Jefferson once states that \"it is wonde...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id                                          full_text\n",
              "0  0000C359D63E  when a person has no experience on a job their...\n",
              "1  000BAD50D026  Do you think students would benefit from being...\n",
              "2  00367BB2546B  Thomas Jefferson once states that \"it is wonde..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submission.shape: (3, 7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
              "0  0000C359D63E       3.0     3.0         3.0          3.0      3.0          3.0\n",
              "1  000BAD50D026       3.0     3.0         3.0          3.0      3.0          3.0\n",
              "2  00367BB2546B       3.0     3.0         3.0          3.0      3.0          3.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n",
        "submission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n",
        "\n",
        "print(f\"test.shape: {test.shape}\")\n",
        "display(test.head())\n",
        "print(f\"submission.shape: {submission.shape}\")\n",
        "display(submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4dd50a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T13:29:31.625437Z",
          "iopub.status.busy": "2022-11-24T13:29:31.625087Z",
          "iopub.status.idle": "2022-11-24T13:29:31.647802Z",
          "shell.execute_reply": "2022-11-24T13:29:31.646893Z"
        },
        "papermill": {
          "duration": 0.06408,
          "end_time": "2022-11-24T13:29:31.649901",
          "exception": false,
          "start_time": "2022-11-24T13:29:31.585821",
          "status": "completed"
        },
        "tags": [],
        "id": "1e4dd50a",
        "outputId": "cb49bc0b-f257-404e-c384-2bdcce7a424d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>tokenize_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>Do you think students would benefit from being...</td>\n",
              "      <td>441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>Thomas Jefferson once states that \"it is wonde...</td>\n",
              "      <td>488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>when a person has no experience on a job their...</td>\n",
              "      <td>897</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id                                          full_text  tokenize_length\n",
              "0  000BAD50D026  Do you think students would benefit from being...              441\n",
              "1  00367BB2546B  Thomas Jefferson once states that \"it is wonde...              488\n",
              "2  0000C359D63E  when a person has no experience on a job their...              897"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test['tokenize_length'] = [len(CFG.tokenizer(text)['input_ids']) for text in test['full_text'].values]\n",
        "test = test.sort_values('tokenize_length', ascending=True).reset_index(drop=True)\n",
        "display(test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83219056",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T13:29:31.731871Z",
          "iopub.status.busy": "2022-11-24T13:29:31.730196Z",
          "iopub.status.idle": "2022-11-24T13:29:31.738174Z",
          "shell.execute_reply": "2022-11-24T13:29:31.737273Z"
        },
        "papermill": {
          "duration": 0.050468,
          "end_time": "2022-11-24T13:29:31.740157",
          "exception": false,
          "start_time": "2022-11-24T13:29:31.689689",
          "status": "completed"
        },
        "tags": [],
        "id": "83219056"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Dataset\n",
        "# ====================================================\n",
        "from dataset import prepare_input, TestDataSet\n",
        "\n",
        "# ====================================================\n",
        "# inference\n",
        "# ====================================================\n",
        "from main_functions import inference_fn\n",
        "\n",
        "from transformers import DataCollatorWithPadding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For final prediction, we average inference from 4 models of cross-validation"
      ],
      "metadata": {
        "id": "cay_9gICDbbE"
      },
      "id": "cay_9gICDbbE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52479d9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T13:29:32.120599Z",
          "iopub.status.busy": "2022-11-24T13:29:32.120305Z",
          "iopub.status.idle": "2022-11-24T13:30:11.953301Z",
          "shell.execute_reply": "2022-11-24T13:30:11.951954Z"
        },
        "papermill": {
          "duration": 39.875806,
          "end_time": "2022-11-24T13:30:11.956780",
          "exception": false,
          "start_time": "2022-11-24T13:29:32.080974",
          "status": "completed"
        },
        "tags": [],
        "id": "b52479d9"
      },
      "outputs": [],
      "source": [
        "test_dataset = TestDataset(CFG, test)\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=CFG.batch_size,\n",
        "                         shuffle=False,\n",
        "                         collate_fn=DataCollatorWithPadding(tokenizer=CFG.tokenizer, padding='longest'),\n",
        "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
        "predictions = []\n",
        "for fold in CFG.trn_fold:\n",
        "    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n",
        "    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
        "                       map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state['model'])\n",
        "    prediction = inference_fn(test_loader, model, device)\n",
        "    predictions.append(prediction)\n",
        "    del model, state, prediction; gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "predictions = np.mean(predictions, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9a1fc6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T13:30:12.077351Z",
          "iopub.status.busy": "2022-11-24T13:30:12.076894Z",
          "iopub.status.idle": "2022-11-24T13:30:12.109623Z",
          "shell.execute_reply": "2022-11-24T13:30:12.108588Z"
        },
        "papermill": {
          "duration": 0.076406,
          "end_time": "2022-11-24T13:30:12.111712",
          "exception": false,
          "start_time": "2022-11-24T13:30:12.035306",
          "status": "completed"
        },
        "tags": [],
        "id": "af9a1fc6",
        "outputId": "612694f4-2170-40f1-c515-0852e2f3a157"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>2.936991</td>\n",
              "      <td>2.822174</td>\n",
              "      <td>3.145676</td>\n",
              "      <td>2.973789</td>\n",
              "      <td>2.714838</td>\n",
              "      <td>2.690477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>2.574048</td>\n",
              "      <td>2.419027</td>\n",
              "      <td>2.668774</td>\n",
              "      <td>2.365274</td>\n",
              "      <td>2.152936</td>\n",
              "      <td>2.551006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>3.459992</td>\n",
              "      <td>3.365598</td>\n",
              "      <td>3.575336</td>\n",
              "      <td>3.514001</td>\n",
              "      <td>3.433239</td>\n",
              "      <td>3.274550</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  conventions\n",
              "0  0000C359D63E  2.936991  2.822174    3.145676     2.973789  2.714838     2.690477\n",
              "1  000BAD50D026  2.574048  2.419027    2.668774     2.365274  2.152936     2.551006\n",
              "2  00367BB2546B  3.459992  3.365598    3.575336     3.514001  3.433239     3.274550"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test[CFG.target_cols] = predictions\n",
        "submission = submission.drop(columns=CFG.target_cols).merge(test[['text_id'] + CFG.target_cols], on='text_id', how='left')\n",
        "display(submission.head())\n",
        "submission[['text_id'] + CFG.target_cols].to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This baseline solution give 0.44 score on a public leaderboard"
      ],
      "metadata": {
        "id": "U8natAPSQfuu"
      },
      "id": "U8natAPSQfuu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf5aae6",
      "metadata": {
        "papermill": {
          "duration": 0.038677,
          "end_time": "2022-11-24T13:30:12.189496",
          "exception": false,
          "start_time": "2022-11-24T13:30:12.150819",
          "status": "completed"
        },
        "tags": [],
        "id": "9bf5aae6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 14886.584906,
      "end_time": "2022-11-24T13:30:15.070563",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-11-24T09:22:08.485657",
      "version": "2.3.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}